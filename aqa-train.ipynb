{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib as plt\n# from transformers import BertTokenizer, TFBertModel\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport os\nos.environ[\"WANDB_API_KEY\"] = \"0\" ## to ıgnore warning","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:19:45.022405Z","iopub.execute_input":"2023-08-09T20:19:45.023469Z","iopub.status.idle":"2023-08-09T20:19:55.870169Z","shell.execute_reply.started":"2023-08-09T20:19:45.023420Z","shell.execute_reply":"2023-08-09T20:19:55.869120Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:98: UserWarning: unable to load libtensorflow_io_plugins.so: unable to open file: libtensorflow_io_plugins.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io_plugins.so: undefined symbol: _ZN3tsl6StatusC1EN10tensorflow5error4CodeESt17basic_string_viewIcSt11char_traitsIcEENS_14SourceLocationE']\n  warnings.warn(f\"unable to load libtensorflow_io_plugins.so: {e}\")\n/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/__init__.py:104: UserWarning: file system plugins are not loaded: unable to open file: libtensorflow_io.so, from paths: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so']\ncaused by: ['/opt/conda/lib/python3.10/site-packages/tensorflow_io/python/ops/libtensorflow_io.so: undefined symbol: _ZTVN10tensorflow13GcsFileSystemE']\n  warnings.warn(f\"file system plugins are not loaded: {e}\")\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# EDA","metadata":{}},{"cell_type":"code","source":"\ntrain = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/train.csv')\ntest = pd.read_csv('/kaggle/input/contradictory-my-dear-watson/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:19:55.872156Z","iopub.execute_input":"2023-08-09T20:19:55.873491Z","iopub.status.idle":"2023-08-09T20:19:56.052111Z","shell.execute_reply.started":"2023-08-09T20:19:55.873446Z","shell.execute_reply":"2023-08-09T20:19:56.050971Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# set GPU\ngpus = tf.config.experimental.list_physical_devices('GPU')\nif gpus:\n    try:\n        for gpu in gpus:\n            tf.config.experimental.set_memory_growth(gpu, True)\n        tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")  # Создаем стратегию для одного GPU\n    except RuntimeError as e:\n        print(e)\nelse:\n    strategy = tf.distribute.get_strategy()\n    print('Number of replicas:', strategy.num_replicas_in_sync)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:19:56.053586Z","iopub.execute_input":"2023-08-09T20:19:56.054052Z","iopub.status.idle":"2023-08-09T20:19:56.081975Z","shell.execute_reply.started":"2023-08-09T20:19:56.054019Z","shell.execute_reply":"2023-08-09T20:19:56.081131Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Number of replicas: 1\n","output_type":"stream"}]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:19:56.084210Z","iopub.execute_input":"2023-08-09T20:19:56.084560Z","iopub.status.idle":"2023-08-09T20:19:56.108765Z","shell.execute_reply.started":"2023-08-09T20:19:56.084532Z","shell.execute_reply":"2023-08-09T20:19:56.107526Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"           id                                            premise  \\\n0  5130fd2cb5  and these comments were considered in formulat...   \n1  5b72532a0b  These are issues that we wrestle with in pract...   \n2  3931fbe82a  Des petites choses comme celles-là font une di...   \n3  5622f0c60b  you know they can't really defend themselves l...   \n4  86aaa48b45  ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...   \n\n                                          hypothesis lang_abv language  label  \n0  The rules developed in the interim were put to...       en  English      0  \n1  Practice groups are not permitted to work on t...       en  English      2  \n2              J'essayais d'accomplir quelque chose.       fr   French      0  \n3  They can't defend themselves because of their ...       en  English      0  \n4    เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร       th     Thai      1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>lang_abv</th>\n      <th>language</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5130fd2cb5</td>\n      <td>and these comments were considered in formulat...</td>\n      <td>The rules developed in the interim were put to...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>5b72532a0b</td>\n      <td>These are issues that we wrestle with in pract...</td>\n      <td>Practice groups are not permitted to work on t...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3931fbe82a</td>\n      <td>Des petites choses comme celles-là font une di...</td>\n      <td>J'essayais d'accomplir quelque chose.</td>\n      <td>fr</td>\n      <td>French</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>5622f0c60b</td>\n      <td>you know they can't really defend themselves l...</td>\n      <td>They can't defend themselves because of their ...</td>\n      <td>en</td>\n      <td>English</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>86aaa48b45</td>\n      <td>ในการเล่นบทบาทสมมุติก็เช่นกัน โอกาสที่จะได้แสด...</td>\n      <td>เด็กสามารถเห็นได้ว่าชาติพันธุ์แตกต่างกันอย่างไร</td>\n      <td>th</td>\n      <td>Thai</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"len(train)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:19:56.110345Z","iopub.execute_input":"2023-08-09T20:19:56.110707Z","iopub.status.idle":"2023-08-09T20:19:56.117629Z","shell.execute_reply.started":"2023-08-09T20:19:56.110676Z","shell.execute_reply":"2023-08-09T20:19:56.116549Z"},"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"12120"},"metadata":{}}]},{"cell_type":"code","source":"train.info()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:19:56.119283Z","iopub.execute_input":"2023-08-09T20:19:56.119728Z","iopub.status.idle":"2023-08-09T20:19:56.177115Z","shell.execute_reply.started":"2023-08-09T20:19:56.119688Z","shell.execute_reply":"2023-08-09T20:19:56.175868Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 12120 entries, 0 to 12119\nData columns (total 6 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   id          12120 non-null  object\n 1   premise     12120 non-null  object\n 2   hypothesis  12120 non-null  object\n 3   lang_abv    12120 non-null  object\n 4   language    12120 non-null  object\n 5   label       12120 non-null  int64 \ndtypes: int64(1), object(5)\nmemory usage: 568.2+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"train.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:19:56.178529Z","iopub.execute_input":"2023-08-09T20:19:56.178910Z","iopub.status.idle":"2023-08-09T20:19:56.211021Z","shell.execute_reply.started":"2023-08-09T20:19:56.178879Z","shell.execute_reply":"2023-08-09T20:19:56.209678Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"id            0\npremise       0\nhypothesis    0\nlang_abv      0\nlanguage      0\nlabel         0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"!pip install plotly","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:19:56.212963Z","iopub.execute_input":"2023-08-09T20:19:56.213424Z","iopub.status.idle":"2023-08-09T20:20:11.705879Z","shell.execute_reply.started":"2023-08-09T20:19:56.213388Z","shell.execute_reply":"2023-08-09T20:20:11.704291Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Requirement already satisfied: plotly in /opt/conda/lib/python3.10/site-packages (5.15.0)\nRequirement already satisfied: tenacity>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from plotly) (8.2.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from plotly) (21.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->plotly) (3.0.9)\n","output_type":"stream"}]},{"cell_type":"code","source":"import plotly.express as px\n\n# Define the data (example data)\nlabels = ['English', 'French', 'Spanish']\nfrequencies = [45, 30, 25]  # Example frequencies for each language\n\n# Define a custom color map for the labels\ncolor_map = {\n    'English': 'rgb(31, 119, 180)',  # Specify your desired color for 'English'\n    'French': 'rgb(255, 127, 14)',   # Specify your desired color for 'French'\n    'Spanish': 'rgb(44, 160, 44)',   # Specify your desired color for 'Spanish'\n    # Add more label-color mappings as needed\n}\n\nfig = px.pie(\n    values=frequencies,\n    names=labels,\n    title='Languages distribution',\n    color_discrete_map=color_map  # Use the custom color map here\n)\n\n# Increase the size of the pie chart\nfig.update_layout(width=800, height=600)\n\nfig.show()\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:11.707842Z","iopub.execute_input":"2023-08-09T20:20:11.708399Z","iopub.status.idle":"2023-08-09T20:20:14.762949Z","shell.execute_reply.started":"2023-08-09T20:20:11.708356Z","shell.execute_reply":"2023-08-09T20:20:14.762008Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/html":"        <script type=\"text/javascript\">\n        window.PlotlyConfig = {MathJaxConfig: 'local'};\n        if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n        if (typeof require !== 'undefined') {\n        require.undef(\"plotly\");\n        requirejs.config({\n            paths: {\n                'plotly': ['https://cdn.plot.ly/plotly-2.24.1.min']\n            }\n        });\n        require(['plotly'], function(Plotly) {\n            window._Plotly = Plotly;\n        });\n        }\n        </script>\n        "},"metadata":{}},{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"c6c78b8e-2ac5-4950-9a97-9f3c221a45e1\" class=\"plotly-graph-div\" style=\"height:600px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"c6c78b8e-2ac5-4950-9a97-9f3c221a45e1\")) {                    Plotly.newPlot(                        \"c6c78b8e-2ac5-4950-9a97-9f3c221a45e1\",                        [{\"domain\":{\"x\":[0.0,1.0],\"y\":[0.0,1.0]},\"hovertemplate\":\"label=%{label}\\u003cbr\\u003evalue=%{value}\\u003cextra\\u003e\\u003c\\u002fextra\\u003e\",\"labels\":[\"English\",\"French\",\"Spanish\"],\"legendgroup\":\"\",\"name\":\"\",\"showlegend\":true,\"values\":[45,30,25],\"type\":\"pie\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"legend\":{\"tracegroupgap\":0},\"title\":{\"text\":\"Languages distribution\"},\"width\":800,\"height\":600},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('c6c78b8e-2ac5-4950-9a97-9f3c221a45e1');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"import plotly.graph_objects as go\n\ntrain['text_length'] = train['premise'].apply(len)\n\nfig = go.Figure(data=[go.Histogram(x=train['text_length'], \n                                   nbinsx=50,\n                                   marker_color='skyblue')])\nfig.update_layout(title_text='Text  distribution in the context of \"premise\"', # title of plot\n                  xaxis_title_text='Len of text in premise', # xaxis label\n                  yaxis_title_text='Number of sentences', # yaxis label\n                  bargap=0.2, # gap between bars of adjacent location coordinates\n                  bargroupgap=0.1) # gap between bars of the same location coordinates\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:14.767638Z","iopub.execute_input":"2023-08-09T20:20:14.768050Z","iopub.status.idle":"2023-08-09T20:20:14.821617Z","shell.execute_reply.started":"2023-08-09T20:20:14.768019Z","shell.execute_reply":"2023-08-09T20:20:14.820090Z"},"trusted":true},"execution_count":10,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"283cdc30-834c-4e70-be44-5b13b4a46546\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"283cdc30-834c-4e70-be44-5b13b4a46546\")) {                    Plotly.newPlot(                        \"283cdc30-834c-4e70-be44-5b13b4a46546\",                        [{\"marker\":{\"color\":\"skyblue\"},\"nbinsx\":50,\"x\":[68,81,92,92,182,147,181,37,78,105,141,35,37,43,125,44,107,65,160,129,134,203,25,57,215,29,21,69,156,21,231,79,43,180,38,125,76,76,157,34,91,68,62,45,87,38,38,95,84,29,29,88,13,221,127,89,106,200,46,126,104,55,29,60,50,70,235,226,173,140,40,225,77,165,98,101,95,242,28,31,125,158,188,85,52,195,107,78,116,111,150,103,47,41,10,109,117,79,156,181,118,16,94,214,114,125,59,123,38,150,117,126,31,124,68,94,120,103,74,73,144,98,184,122,146,88,107,20,193,82,48,117,46,21,76,208,109,147,135,128,77,159,191,41,35,80,165,68,57,101,151,152,160,97,267,103,91,107,157,29,226,63,146,55,107,122,110,17,76,151,125,42,83,112,77,29,182,67,145,46,57,59,277,51,58,71,194,313,145,83,270,70,66,60,12,47,24,74,38,45,78,154,125,16,118,38,283,141,104,82,97,171,74,64,75,148,107,81,135,261,192,87,249,100,44,22,100,73,13,43,124,196,172,75,42,53,140,83,208,33,52,154,116,153,97,61,156,20,126,100,111,239,40,35,99,102,24,31,76,20,76,9,28,69,53,44,43,128,19,293,116,87,193,133,91,227,54,156,157,184,14,44,132,82,25,167,15,44,23,50,19,30,97,40,148,31,146,117,118,101,190,105,119,232,111,53,85,76,23,9,81,73,109,124,127,98,34,213,41,216,198,97,155,216,89,119,105,78,102,21,183,102,328,107,61,66,16,115,33,61,93,192,24,83,50,70,146,61,134,19,10,33,104,73,29,73,126,57,71,92,137,169,167,98,37,41,108,98,97,73,158,129,154,53,192,38,49,51,29,106,67,60,130,130,210,33,67,148,161,153,241,55,158,13,156,92,86,21,206,135,57,85,82,32,149,54,143,115,126,29,83,154,18,63,57,44,49,61,44,54,100,13,92,75,93,207,89,119,111,76,52,146,51,52,89,53,169,126,53,41,50,45,182,81,94,99,64,87,144,85,70,130,52,70,29,87,93,168,99,61,46,149,103,81,50,63,102,56,112,73,74,13,44,114,149,156,85,152,76,40,19,42,34,66,53,19,110,231,261,65,76,117,54,40,271,127,99,301,111,79,199,75,88,96,162,13,91,39,191,233,75,168,38,131,134,33,26,191,12,187,118,115,164,120,79,39,139,115,199,47,204,74,132,271,115,112,114,215,23,331,50,162,70,197,21,116,323,81,149,177,77,92,140,24,86,233,37,47,139,103,160,24,52,33,65,72,140,162,27,110,176,184,86,185,182,76,85,148,112,102,152,39,43,70,208,84,124,47,47,178,32,97,117,161,38,142,26,103,107,161,172,82,139,124,24,111,142,111,13,108,73,134,219,52,91,133,35,63,69,30,129,84,136,17,88,61,91,116,62,183,24,116,80,142,77,135,42,34,112,22,125,71,165,54,127,26,173,179,60,102,67,119,208,45,46,184,45,206,158,80,291,31,137,37,97,159,115,109,141,20,64,182,13,109,14,100,201,36,22,367,233,106,185,67,149,137,196,89,12,46,64,148,60,22,88,65,44,49,82,91,135,26,98,49,148,14,83,88,116,72,188,237,210,38,67,19,42,82,54,99,270,29,73,107,13,53,100,55,46,163,110,115,178,169,164,54,52,19,59,207,110,75,89,40,137,152,57,79,73,66,159,131,185,113,176,28,34,147,190,56,102,131,83,63,33,55,20,76,53,65,44,106,74,11,41,177,123,73,97,251,49,11,67,135,224,54,115,50,65,33,36,83,49,212,49,64,182,80,112,127,62,140,63,122,80,76,177,90,109,240,68,43,72,128,73,85,34,94,120,35,52,217,30,120,184,121,113,46,159,44,159,87,35,63,107,101,124,172,110,40,72,85,102,178,34,36,129,39,57,387,99,106,46,221,51,44,51,90,65,86,84,62,131,79,157,173,60,159,100,106,65,182,125,189,527,38,111,174,44,25,191,144,165,112,133,115,37,40,158,146,62,87,69,87,113,63,186,94,178,107,83,19,66,108,74,89,81,132,132,253,104,180,115,73,141,95,144,260,271,37,74,67,73,97,150,50,163,9,159,41,173,24,206,20,203,95,87,88,64,92,31,158,170,125,65,72,40,179,79,56,17,99,53,55,94,28,35,153,39,80,32,28,32,40,88,286,215,23,75,58,64,123,97,52,99,67,191,20,89,158,127,28,58,45,237,51,63,191,64,124,57,64,205,113,92,156,216,151,140,158,72,83,43,47,186,46,32,110,73,147,65,170,121,107,83,233,130,209,110,36,192,79,149,171,25,110,113,60,37,34,21,84,33,131,217,72,161,89,13,52,35,178,140,46,201,67,153,166,143,20,162,13,68,123,69,88,72,152,15,210,186,175,37,132,80,81,118,30,252,77,66,41,54,37,176,33,183,196,55,78,151,174,231,220,174,105,126,205,136,47,26,57,114,105,224,237,113,84,32,201,38,146,167,82,102,26,49,26,80,157,132,165,77,85,72,203,89,13,93,47,46,151,167,320,22,60,105,117,56,51,126,93,47,44,174,232,109,56,98,20,47,40,59,27,73,81,96,37,84,108,16,273,50,114,122,35,39,226,51,37,47,346,208,264,55,92,78,13,178,73,39,107,70,76,87,143,66,84,105,42,163,120,140,162,85,216,177,117,44,165,59,163,68,52,132,148,136,172,54,22,102,57,108,35,262,30,158,151,196,75,84,27,55,71,56,117,65,41,237,53,109,129,37,59,186,67,94,159,134,39,28,137,33,143,77,53,158,207,97,56,34,20,167,31,68,139,182,135,162,117,56,184,100,49,122,106,73,40,138,114,99,72,110,109,180,72,37,233,40,54,18,91,196,86,246,110,135,132,27,143,198,28,142,115,36,98,144,177,80,54,30,124,122,130,195,111,61,217,156,133,145,210,231,44,64,66,91,123,201,88,86,146,52,191,166,71,140,46,120,125,208,254,55,26,252,37,37,146,242,28,24,272,30,130,54,121,55,22,94,27,99,184,143,25,111,99,70,140,71,102,82,27,66,103,115,401,73,55,151,121,110,33,74,15,128,192,152,144,39,155,170,12,79,104,213,109,112,20,175,151,117,205,20,193,153,143,13,92,259,159,147,65,90,61,80,174,160,26,77,15,32,52,111,112,73,57,121,166,101,81,38,146,23,42,220,140,10,69,132,112,22,58,21,198,143,18,252,125,271,107,177,23,63,37,122,45,95,128,26,51,102,48,439,210,122,125,176,176,21,92,170,78,195,84,106,74,95,26,243,99,112,78,127,131,188,120,164,67,177,100,134,129,28,155,188,53,91,243,58,237,174,91,40,78,46,122,187,66,145,108,99,115,254,135,61,174,170,134,121,19,151,130,20,72,78,122,141,168,70,241,185,38,93,136,71,67,15,134,135,217,92,71,14,203,103,58,42,45,45,161,33,129,128,96,102,106,13,51,241,88,167,118,24,57,32,27,64,152,52,35,181,52,49,23,83,74,51,118,29,115,140,153,43,99,165,109,175,154,195,160,17,28,108,59,55,77,90,107,49,69,111,61,91,39,104,153,184,139,34,252,114,187,52,122,141,182,159,87,20,150,126,47,151,114,49,61,103,29,100,207,48,33,37,64,107,162,111,91,196,346,126,86,51,33,76,129,151,40,158,90,210,73,192,34,96,241,54,61,75,213,111,105,65,211,118,124,170,119,146,109,124,141,127,154,211,69,52,105,165,96,104,62,38,75,112,125,98,39,22,158,122,249,42,177,208,120,89,44,100,307,109,114,121,62,148,256,116,134,130,156,108,64,48,392,61,178,39,84,37,58,234,126,22,40,203,52,142,241,23,31,94,32,233,57,283,205,265,32,98,114,124,55,131,165,11,200,160,62,89,141,90,162,9,82,124,42,68,211,40,73,160,232,185,70,201,170,23,155,103,63,44,53,62,63,46,115,45,48,136,64,50,44,88,68,71,153,105,114,131,62,115,144,119,78,27,81,130,20,144,37,111,14,112,53,18,197,39,60,131,35,54,50,36,127,61,132,88,65,54,171,154,78,22,181,193,137,37,189,111,106,82,88,208,29,202,26,212,99,34,152,139,10,126,179,78,109,45,24,22,52,195,110,100,208,144,42,27,58,151,73,241,114,131,54,557,61,67,118,82,60,39,79,42,167,106,141,25,67,104,107,227,75,125,56,173,158,72,80,92,180,104,101,243,55,52,13,63,134,131,110,28,33,58,141,18,107,89,125,46,34,20,50,131,159,139,41,83,98,19,107,84,164,103,195,107,103,79,93,60,211,126,42,72,155,75,280,63,27,111,9,120,198,77,171,193,189,134,259,100,37,164,120,78,109,170,269,118,141,155,98,198,161,220,202,76,319,59,69,74,33,110,264,151,157,129,131,74,124,72,88,169,52,179,91,82,62,130,224,54,89,27,294,117,191,67,146,88,89,75,106,47,36,68,94,43,62,201,255,44,108,62,128,29,54,81,102,72,172,122,42,92,20,41,45,94,175,139,175,156,187,94,145,41,18,91,93,149,24,27,20,168,176,116,70,83,39,85,21,97,232,159,31,88,193,37,73,136,47,78,43,182,182,153,44,208,251,90,229,105,80,113,69,153,167,116,72,26,81,112,195,88,230,60,99,56,75,302,38,225,130,235,25,275,46,173,101,127,106,13,28,95,26,165,89,50,194,210,190,195,164,56,136,88,76,42,248,425,72,81,104,110,162,72,39,45,167,152,154,180,156,109,25,30,85,80,221,167,79,82,189,23,95,161,62,137,112,35,132,61,85,60,74,30,36,100,52,55,54,28,27,119,49,22,203,245,102,144,35,164,61,77,64,19,31,59,146,107,172,85,275,33,49,122,171,213,191,57,249,124,117,18,8,124,65,53,149,90,131,120,54,215,94,202,189,23,167,114,72,17,271,27,95,154,115,111,190,166,128,150,43,50,99,145,132,37,78,150,45,217,85,52,114,185,99,137,60,63,144,85,146,179,187,113,85,79,54,105,154,134,158,129,57,82,37,152,25,233,27,37,213,107,198,110,117,744,43,58,51,176,275,75,73,20,35,15,109,89,112,51,113,47,71,114,28,87,115,195,165,46,164,43,198,46,32,148,68,106,143,67,25,114,37,185,185,141,68,34,77,115,47,199,133,19,216,124,50,70,45,105,44,66,157,217,33,201,10,121,85,200,40,331,43,43,37,54,37,218,23,92,34,40,104,144,101,42,69,130,150,133,340,30,75,37,201,142,96,95,147,29,241,152,39,84,54,55,128,30,120,10,80,37,83,91,59,78,33,90,156,84,140,50,109,99,14,58,160,25,150,147,67,230,153,88,119,131,120,118,33,118,62,49,276,122,219,89,168,339,233,41,160,309,106,66,44,223,13,71,38,90,14,88,177,124,100,138,55,226,137,82,148,237,191,124,177,119,82,59,177,157,26,57,221,197,98,133,172,161,35,60,49,77,182,110,88,65,36,57,39,68,202,179,170,42,102,128,85,73,18,122,163,31,16,75,121,72,200,97,88,89,4,132,160,72,100,66,165,136,66,155,195,105,151,173,25,145,282,200,21,59,65,146,146,37,29,94,70,80,289,117,23,160,116,112,76,122,82,83,50,177,110,170,62,108,54,9,67,43,67,118,40,53,59,41,73,65,137,48,106,51,95,109,108,13,124,123,108,109,61,79,66,101,29,29,282,87,88,58,28,195,45,127,23,32,195,41,20,151,85,173,69,135,176,38,46,97,45,103,196,22,171,95,91,36,165,200,131,28,49,31,149,179,163,86,212,148,143,79,136,220,186,74,48,128,146,86,126,125,78,68,61,64,399,225,166,100,179,149,115,127,30,160,35,186,291,136,56,192,106,77,44,114,237,185,135,158,106,88,93,57,73,114,44,64,142,131,42,121,137,95,21,45,127,111,34,74,192,130,49,74,220,132,129,55,148,212,52,165,124,114,51,216,29,34,266,129,203,166,216,184,49,51,27,16,88,46,109,138,51,227,104,44,139,163,113,129,62,57,55,95,47,173,70,196,152,31,60,126,133,180,120,120,28,249,162,157,109,24,25,82,95,120,80,98,14,168,165,425,43,115,157,139,62,96,95,62,76,100,106,46,91,45,36,163,86,66,141,242,68,151,115,55,175,66,57,126,158,103,59,82,50,69,209,22,136,22,88,41,15,39,141,114,37,83,69,45,60,173,64,148,129,91,109,362,51,102,109,20,29,65,134,183,66,37,43,94,102,54,65,165,122,118,148,72,145,157,69,171,103,48,126,114,56,66,84,209,169,277,54,216,80,23,36,18,134,300,59,48,154,124,110,43,67,157,134,50,28,118,227,84,85,18,129,151,98,116,10,129,85,123,59,84,83,26,81,93,145,146,220,146,37,365,27,45,178,128,162,89,65,196,92,60,64,56,26,213,18,95,179,77,146,50,182,182,72,111,144,105,52,85,74,46,186,50,159,173,69,282,104,273,12,115,94,90,23,455,105,174,151,111,266,28,281,121,127,130,157,90,146,55,205,197,98,95,34,271,64,57,202,120,48,59,177,68,71,110,16,234,161,162,86,120,146,170,133,151,215,87,55,185,107,119,56,28,44,175,144,58,203,154,34,41,199,62,85,219,80,104,88,97,61,43,108,154,194,46,200,71,105,121,74,28,19,59,131,181,95,77,85,148,68,88,94,77,105,104,45,37,66,67,273,158,103,55,113,175,51,84,45,164,99,22,82,102,174,29,119,56,102,41,57,212,51,205,40,199,113,198,135,104,97,23,143,161,57,46,144,63,93,233,48,181,18,197,13,107,192,248,55,123,99,143,66,157,55,182,21,358,145,102,179,92,159,214,99,39,106,155,220,161,86,34,103,106,256,146,97,273,105,251,60,165,63,250,108,62,255,145,99,84,286,128,75,150,41,79,134,154,146,76,18,226,85,177,55,185,97,261,171,208,122,259,35,76,143,205,189,63,52,191,119,51,246,82,99,74,89,21,62,101,120,205,127,81,89,184,98,75,48,44,42,116,201,98,65,208,55,49,217,44,131,71,109,21,119,155,132,150,108,81,20,154,20,176,140,97,91,246,127,118,112,106,97,26,100,163,115,220,74,39,19,76,131,27,161,77,32,94,173,99,13,134,148,137,101,198,157,75,155,245,52,115,60,40,191,83,64,98,143,117,16,148,149,11,48,88,229,141,176,86,120,52,16,168,58,237,52,88,33,28,124,46,154,80,107,29,152,100,130,159,258,282,189,77,62,36,193,112,204,49,130,239,92,58,54,221,130,34,60,65,137,62,116,141,106,16,228,203,22,46,123,34,110,236,88,19,46,203,39,154,181,70,182,123,180,23,28,99,85,41,124,195,94,64,23,145,10,124,116,154,77,64,47,99,80,168,54,144,46,43,91,42,54,93,226,147,127,229,136,41,88,84,83,50,84,216,40,154,202,29,30,201,82,188,177,100,24,120,76,88,37,154,65,82,188,119,182,112,83,106,73,258,182,163,115,129,207,36,216,56,40,142,50,21,94,35,187,9,113,89,79,9,60,64,60,16,48,93,107,87,137,104,216,112,20,116,119,63,105,158,172,31,66,127,163,108,137,29,39,127,229,62,182,13,26,44,136,121,335,172,8,218,20,144,66,23,29,66,46,43,160,345,109,67,138,88,32,41,164,344,213,61,45,40,271,38,78,134,150,47,73,100,31,118,73,187,218,274,129,80,118,21,75,233,100,26,56,151,144,95,23,92,63,40,23,51,33,144,23,27,80,53,88,76,154,145,40,70,188,194,174,191,82,61,56,16,99,50,120,64,92,88,128,128,24,78,52,34,31,61,64,72,37,158,64,56,86,202,134,157,161,108,171,52,102,198,45,85,75,133,130,45,199,48,91,122,101,27,127,141,65,227,122,199,60,17,252,131,20,137,126,146,65,133,210,224,147,91,49,92,70,93,57,61,109,30,254,128,115,63,109,39,54,391,203,81,39,169,53,68,149,104,109,100,182,87,69,101,285,140,35,80,95,31,107,29,237,58,107,96,154,169,116,117,77,49,27,87,54,55,57,132,43,154,132,25,179,40,85,88,89,139,84,230,93,98,19,29,41,19,90,78,101,78,136,41,154,141,215,22,25,142,228,138,45,205,211,63,157,159,58,87,139,37,33,70,120,58,42,187,207,115,121,49,13,71,169,123,208,151,60,207,166,70,399,122,84,35,144,149,67,21,168,144,122,105,23,32,126,150,90,229,210,53,70,26,203,109,77,51,75,162,31,53,217,171,228,230,204,119,47,246,195,71,178,218,56,169,113,157,60,232,63,79,185,109,170,12,233,72,135,171,9,109,33,20,144,147,177,186,106,54,13,17,71,112,96,91,18,95,166,139,208,135,108,185,258,58,171,117,45,23,273,196,86,40,28,168,403,112,166,162,137,79,144,101,46,59,132,95,136,163,181,173,33,280,117,155,105,46,43,200,131,108,77,72,331,164,32,57,144,151,149,57,81,79,250,101,209,115,179,90,36,16,74,123,140,88,178,95,204,125,123,54,214,276,47,74,88,51,295,162,98,126,291,75,14,63,81,20,143,241,72,71,32,56,35,55,219,134,180,163,122,96,83,128,98,181,99,105,24,45,180,20,92,61,78,25,95,79,62,45,45,62,104,54,100,144,137,85,70,105,94,95,33,126,125,59,81,190,149,60,126,124,80,94,81,153,160,34,159,66,125,102,137,138,42,44,25,29,26,38,212,26,13,252,115,49,403,131,83,164,35,86,34,29,84,104,219,123,83,222,69,134,71,127,136,150,165,183,159,132,99,77,214,150,66,96,94,144,64,158,345,61,42,123,90,87,40,107,106,104,29,81,78,65,73,164,98,141,148,168,104,26,128,173,172,50,93,12,37,702,43,42,24,77,39,103,120,165,87,89,126,75,144,131,67,13,97,87,31,131,40,274,99,73,41,161,91,190,63,65,115,32,133,174,96,309,159,81,132,35,92,41,101,20,91,15,61,64,96,138,125,94,58,23,172,95,183,172,126,113,116,194,164,64,56,107,226,128,225,145,134,39,150,64,55,254,70,40,57,135,202,174,37,164,162,43,55,106,24,148,58,81,99,103,42,222,37,39,205,59,14,145,253,110,65,109,91,107,142,65,101,106,34,218,195,61,76,87,169,34,22,138,307,60,34,54,42,111,175,29,125,20,125,108,36,159,59,219,42,73,76,41,158,125,97,71,20,180,137,65,66,269,156,45,105,33,212,256,58,90,196,110,128,107,93,288,127,71,80,58,72,159,156,216,88,136,128,24,112,76,84,87,35,51,78,214,160,39,69,23,74,67,68,115,80,49,94,28,26,79,106,182,82,49,133,62,78,64,30,133,207,111,145,38,58,188,162,116,10,257,104,233,127,256,158,141,67,126,107,190,18,129,160,115,23,68,261,180,153,38,212,32,91,101,137,134,153,122,106,195,19,110,72,42,80,55,96,16,223,136,335,58,173,122,253,127,40,161,20,54,99,173,89,34,82,102,24,99,124,181,223,185,77,132,81,19,243,167,198,91,32,98,80,101,183,38,149,97,153,56,17,180,148,146,181,176,114,102,77,92,69,84,184,135,247,132,110,63,103,51,68,137,186,157,74,124,159,76,241,76,206,89,83,76,148,15,76,37,85,23,26,66,177,135,51,96,77,95,178,169,43,247,187,41,60,25,144,34,102,10,155,16,32,132,123,144,119,68,86,98,43,40,157,102,260,105,66,97,28,103,136,214,184,54,114,48,56,98,115,81,34,55,125,194,135,42,25,82,84,130,86,96,108,40,53,139,86,64,73,102,92,49,44,149,134,133,137,120,50,83,122,106,154,181,56,66,110,143,226,71,67,113,34,77,206,93,37,67,271,108,140,130,193,169,218,178,58,96,207,165,76,67,120,231,31,51,56,103,83,183,19,137,125,101,104,55,102,82,157,28,98,89,113,81,215,10,76,136,158,15,108,266,15,103,147,80,68,192,153,83,344,210,125,113,47,29,42,105,174,197,16,148,20,60,259,104,115,20,78,112,47,50,74,142,183,51,16,52,148,58,39,130,199,151,40,80,105,106,84,200,39,95,134,153,197,85,92,111,59,26,178,133,23,93,74,266,191,60,17,30,124,180,129,117,147,78,12,202,175,95,41,166,18,129,39,43,9,43,71,149,93,139,100,149,133,109,67,96,22,45,125,241,131,45,163,119,33,127,50,155,196,115,24,115,51,44,125,71,82,192,104,65,41,106,80,133,93,115,26,86,113,173,97,74,87,66,63,185,85,58,101,139,128,153,201,92,75,67,65,107,77,87,215,190,39,160,131,173,158,85,42,45,145,28,52,24,246,37,160,25,113,268,45,206,90,9,268,180,35,49,90,45,84,97,97,22,124,60,65,203,119,104,28,128,39,25,225,159,39,15,18,75,38,41,87,60,197,9,196,28,59,141,125,169,185,251,37,116,73,170,91,209,122,15,186,111,76,61,36,200,65,73,97,24,157,146,19,36,40,168,147,26,53,27,10,191,106,229,39,80,102,35,119,164,94,138,37,31,43,117,205,28,97,42,132,37,153,73,62,165,98,88,13,22,87,63,134,56,104,424,21,178,35,125,76,184,86,195,98,62,48,232,192,32,202,90,233,43,70,114,13,98,93,124,122,131,130,164,142,392,181,179,242,127,77,104,216,154,88,16,38,152,110,186,182,195,75,9,124,45,78,25,114,21,21,56,82,35,104,148,107,86,81,78,104,296,104,246,134,34,85,34,162,92,101,53,76,88,148,176,185,135,85,133,117,54,90,94,51,60,161,100,99,19,31,228,230,84,156,139,45,216,84,133,68,163,349,174,510,193,199,100,51,130,172,58,158,108,203,243,97,67,122,86,170,18,48,61,132,175,75,345,967,191,95,62,90,175,83,152,51,44,175,60,29,39,83,557,20,98,130,45,74,178,89,52,218,36,205,66,75,42,73,149,156,53,172,61,83,66,230,173,131,154,156,397,96,74,44,56,25,77,39,178,40,82,36,189,176,121,112,63,163,18,170,61,282,196,224,61,88,88,172,50,91,158,91,128,58,109,179,38,29,129,92,87,43,200,64,43,108,123,33,139,266,80,14,32,114,43,271,255,132,94,173,23,233,143,128,114,214,204,93,175,102,189,108,59,48,46,12,103,104,137,41,143,127,52,129,29,86,70,109,102,18,87,121,134,84,91,119,47,109,123,137,58,54,240,209,68,233,23,23,26,13,83,66,133,185,114,152,68,163,70,43,70,296,44,62,145,568,95,87,81,33,38,97,44,49,59,128,40,154,75,31,35,88,208,48,124,20,120,114,145,22,139,16,202,85,51,177,50,159,182,30,216,191,146,32,51,88,93,119,126,171,154,117,167,193,35,190,167,288,141,39,151,125,125,154,13,47,103,69,160,159,40,73,93,215,108,134,99,86,27,52,105,144,137,78,37,8,19,271,101,71,74,21,135,14,229,102,88,82,190,73,140,76,103,39,53,28,432,161,62,173,33,48,122,104,87,56,163,178,93,182,80,96,132,109,195,125,155,190,183,302,15,67,38,63,44,57,220,173,80,83,23,70,39,157,109,26,101,190,97,15,192,136,196,121,90,71,184,89,178,40,106,107,131,53,110,61,193,87,89,77,91,53,60,287,76,102,21,87,36,115,170,59,36,97,24,146,149,51,28,527,62,137,29,21,35,48,557,135,127,115,113,37,114,196,37,24,41,149,52,84,270,70,71,68,79,173,207,98,193,34,46,139,167,176,31,66,83,183,150,30,20,169,97,192,137,132,63,77,76,28,105,79,152,100,147,125,124,69,211,185,146,138,31,163,59,91,293,58,87,42,88,131,86,205,118,172,229,205,89,66,59,140,28,122,82,109,54,65,109,36,45,247,50,20,21,36,228,47,94,48,31,84,87,160,151,126,201,147,147,157,123,147,130,50,13,28,74,120,195,213,237,30,106,128,129,216,144,100,145,43,47,167,64,307,268,45,12,100,18,155,596,28,101,26,205,95,141,166,62,199,124,36,53,76,179,40,56,146,168,41,103,22,341,269,109,147,55,114,31,168,35,100,159,70,187,11,192,139,68,230,29,102,42,18,55,19,37,89,70,42,76,89,104,32,21,22,144,27,395,87,83,32,55,64,199,40,96,207,91,118,111,27,61,19,123,47,102,75,55,59,136,73,116,98,28,134,56,156,131,152,206,203,104,80,135,118,75,115,12,222,46,178,78,259,100,44,48,109,87,30,97,33,42,146,158,32,21,50,108,53,71,399,149,42,35,123,48,46,26,141,70,87,52,42,157,47,151,100,32,178,128,180,15,22,82,33,82,29,80,149,163,42,178,199,30,43,150,107,123,82,67,38,80,107,13,98,99,43,136,167,174,133,107,68,22,124,36,55,527,44,276,58,248,126,115,96,89,40,156,35,340,115,177,146,102,15,103,74,45,262,34,36,49,117,140,12,152,28,30,28,82,128,339,157,163,93,73,16,323,104,45,18,72,129,76,89,53,191,104,42,142,183,41,26,246,151,23,90,86,16,79,151,199,19,16,46,206,21,92,31,110,32,138,168,135,90,76,40,57,106,29,178,157,51,50,93,145,16,110,118,88,169,33,57,47,104,120,45,153,208,128,30,32,60,98,30,54,126,82,48,122,111,17,60,61,155,233,175,51,42,216,27,269,164,125,158,275,163,103,38,130,45,103,28,101,47,52,197,79,130,87,34,33,146,98,73,38,199,72,106,67,29,95,60,182,179,114,27,36,96,13,219,281,28,121,967,118,119,190,200,85,69,85,269,122,12,84,137,158,208,160,80,18,114,224,196,104,13,15,39,95,13,101,42,182,23,42,198,40,64,20,88,139,117,39,50,77,211,118,214,115,120,54,89,99,208,107,28,75,32,189,178,96,61,51,145,77,68,39,43,144,146,27,23,40,77,73,63,113,160,223,28,37,174,124,144,114,158,49,101,175,39,139,89,79,58,88,189,76,196,43,53,40,108,64,20,87,161,213,110,141,164,90,70,125,79,110,94,164,148,38,50,12,42,49,23,180,104,59,142,31,141,35,120,178,110,43,9,307,133,237,213,10,157,103,213,23,95,89,16,109,127,137,351,127,128,50,30,233,45,171,90,18,75,119,72,104,184,144,131,23,43,178,9,68,100,257,68,55,251,93,185,203,100,63,82,214,93,158,171,227,75,185,27,123,181,199,137,152,79,46,196,48,61,133,117,74,183,58,205,128,144,100,88,112,41,58,120,50,65,19,125,225,185,76,10,119,59,75,165,70,151,193,164,179,39,131,88,103,149,134,135,179,270,232,115,157,78,195,59,85,63,190,102,39,188,30,160,94,136,78,120,196,77,133,82,58,146,40,14,140,105,56,195,57,189,109,169,150,107,108,107,199,33,103,118,56,280,165,32,93,100,43,214,91,158,40,153,45,78,112,218,67,120,72,57,66,154,178,57,39,244,45,72,77,139,116,37,174,73,183,200,109,224,67,36,103,67,50,143,35,58,93,94,145,199,174,78,53,216,116,111,19,58,208,97,112,139,163,40,311,38,71,68,40,171,162,70,188,134,127,196,76,40,31,210,84,167,43,50,44,84,110,140,106,24,165,55,46,109,79,125,117,237,84,81,97,124,15,177,33,243,36,96,164,74,146,192,74,65,201,44,98,95,20,26,90,135,294,199,141,34,32,37,121,70,126,203,42,41,182,170,60,28,63,65,108,118,16,201,33,193,62,151,93,143,155,342,46,114,167,143,99,121,133,136,112,232,58,26,268,107,73,105,26,57,164,33,115,48,96,63,111,247,23,93,295,65,193,212,43,47,178,199,14,79,53,115,87,160,111,26,202,78,79,160,139,134,45,165,127,216,93,89,42,358,133,120,134,33,124,55,109,42,63,207,20,88,60,47,67,163,84,176,230,134,144,157,55,134,172,74,439,157,307,56,119,71,56,61,54,39,88,120,77,173,39,144,113,127,66,160,31,241,23,73,79,78,126,20,166,40,64,106,86,64,120,166,153,76,147,180,75,236,30,176,110,79,38,137,127,209,10,71,198,111,90,26,68,98,42,15,164,24,174,180,97,111,157,55,40,22,68,182,64,151,183,403,21,74,67,127,11,52,18,38,148,110,54,65,14,42,187,274,95,49,134,100,48,160,37,128,73,133,80,88,233,80,25,141,96,82,78,122,36,87,26,27,63,92,123,38,106,196,76,25,171,232,224,101,28,65,85,133,84,207,114,49,210,113,80,103,111,49,39,111,115,179,96,98,291,135,54,104,268,45,201,24,67,62,181,196,160,152,43,83,85,151,83,55,171,184,29,168,103,55,114,35,142,75,99,168,140,46,63,74,29,33,123,149,166,53,131,199,112,103,63,54,67,94,154,67,31,83,72,21,141,317,35,158,126,96,14,172,122,103,123,19,62,80,28,40,24,264,176,47,177,55,58,82,61,178,40,95,174,26,106,78,105,178,117,90,154,133,32,88,45,241,141,230,42,56,89,171,21,135,97,78,18,39,75,41,72,28,51,7,97,43,22,106,69,180,12,16,9,225,140,91,92,170,189,26,39,171,23,156,101,57,702,45,61,38,72,157,123,181,84,60,96,112,48,60,115,55,72,188,33,129,103,51,73,46,217,6,198,159,155,14,130,130,130,65,223,61,176,19,51,167,89,102,119,86,38,31,185,59,29,48,224,148,125,32,81,133,178,219,30,148,70,54,143,90,98,34,89,201,111,55,98,73,90,41,147,121,135,78,56,182,151,92,237,225,81,65,42,48,60,147,72,110,65,140,179,113,40,167,109,46,153,105,397,53,48,116,30,21,55,74,31,93,76,103,121,76,125,77,84,82,60,94,241,35,90,76,83,178,158,59,66,161,81,287,109,178,55,108,237,237,12,32,142,256,60,130,130,167,131,116,85,62,103,143,90,104,216,198,24,60,85,32,196,24,179,87,70,63,162,100,79,208,205,40,33,105,99,34,104,30,44,138,323,233,202,100,27,49,92,57,192,215,188,30,117,158,111,102,152,15,14,178,111,47,100,46,100,92,77,69,127,64,233,78,46,160,22,191,97,27,80,194,62,47,45,143,235,39,49,175,65,9,27,99,148,53,76,151,68,596,151,75,27,161,20,36,40,62,52,77,12,95,158,150,76,180,119,19,95,52,73,347,126,54,155,131,42,155,26,192,75,85,50,99,65,164,52,160,24,65,28,243,119,290,87,67,31,47,19,54,113,74,62,79,22,61,52,112,32,187,149,177,20,68,141,149,25,72,169,54,80,26,179,131,26,145,226,196,57,155,224,97,30,67,151,119,192,111,173,219,76,42,77,97,42,37,120,335,115,90,134,128,223,97,69,127,123,22,134,154,85,40,36,65,47,76,251,249,68,176,94,135,98,142,193,118,149,100,33,43,165,37,54,60,209,47,111,167,57,126,149,89,20,156,138,125,27,66,116,120,137,165,768,199,148,70,96,254,20,37,74,146,68,39,38,24,113,22,47,75,90,164,14,37,177,213,67,68,122,132,85,30,86,114,115,84,312,164,76,72,104,115,72,193,85,95,212,72,43,55,120,104,57,69,23,131,84,892,153,87,223,144,78,132,64,41,109,39,61,81,60,111,71,88,130,335,153,83,106,51,52,51,42,124,40,75,93,166,189,102,121,146,139,30,216,42,159,47,143,64,28,112,39,93,146,124,53,62,63,63,70,39,60,133,95,137,104,219,358,186,113,91,55,75,82,167,130,140,103,158,124,86,164,41,31,136,85,98,44,193,70,142,188,86,50,276,60,33,35,99,124,250,32,365,125,91,67,103,177,73,94,132,60,37,273,34,68,112,86,29,126,51,83,130,29,145,111,132,260,187,43,54,76,148,8,35,138,233,60,107,45,44,203,64,134,37,103,155,55,20,62,118,79,86,70,339,187,101,111,103,147,84,81,71,19,126,45,75,143,120,115,146,253,128,60,80,147,160,60,69,155,117,53,72,166,129,90,23,64,56,85,111,15,67,131,67,54,76,101,208,132,173,148,148,225,108,44,28,117,39,56,180,138,36,48,24,83,16,20,153,78,180,51,103,46,42,105,17,135,90,83,81,75,91,29,109,114,139,41,20,54,135,138,42,93,84,264,143,205,82,182,122,60,55,147,152,130,64,75,57,205,146,58,53,18,114,42,189,44,49,110,41,144,262,129,23,42,16,152,107,81,193,283,105,159,165,30,79,38,67,96,31,196,80,56,97,39,77,120,129,180,72,45,152,22,87,71,178,43,87,26,20,83,231,113,83,46,59,113,215,256,97,215,99,179,27,77,162,125,94,179,93,81,76,186,162,109,41,61,140,48,111,138,141,159,66,207,152,128,70,91,25,256,153,47,53,127,131,108,17,61,99,68,17,120,142,78,113,119,131,42,184,31,32,107,165,88,146,59,74,159,40,49,48,29,278,9,94,97,109,27,173,124,62,162,69,33,74,75,80,59,131,84,32,78,116,250,54,99,31,206,99,157,219,54,130,213,149,109,128,73,173,29,176,25,42,181,97,52,167,90,133,18,82,112,159,138,153,77,133,73,65,48,69,146,31,167,135,133,87,137,176,64,96,65,744,105,76,94,93,250,85,70,64,32,208,96,101,59,13,58,148,155,152,168,122,81,13,80,164,72,91,25,13,91,209,191,323,71,116,147,31,13,228,143,65,111,61,23,61,281,82,109,86,96,86,119,45,86,118,122,200,47,87,200,68,95,58,48,58,23,11,13,43,26,186,20,30,70,131,225,201,56,152,120,24,207,39,87,195,53,28,108,125,93,77,63,44,38,24,62,45,213,170,30,93,242,341,36,25,84,140,62,109,139,144,100,33,145,13,148,133,122,103,93,76,130,26,70,63,88,89,132,125,112,93,33,92,47,138,38,185,32,128,28,104,63,139,138,36,130,102,52,93,48,200,53,107,206,84,215,110,81,108,112,24,98,193,24,30,205,58,191,78,125,90,138,105,58,81,122,79,367,159,149,13,35,102,33,92,173,66,155,132,97,170,154,158,162,20,64,79,145,40,123,29,105,157,72,118,38,227,217,200,127,48,131,107,282,129,129,55,63,34,98,67,130,97,61,124,193,76,136,44,63,90,79,161,204,131,76,16,200,30,70,198,83,260,58,108,262,82,70,153,55,272,146,171,39,37,95,106,124,129,169,103,184,122,155,50,50,40,51,14,70,156,29,95,49,250,100,109,73,45,66,39,48,43,73,49,84,25,28,12,157,27,82,153,198,148,121,140,233,302,198,124,137,29,67,215,134,118,147,115,140,32,66,20,152,24,76,46,40,46,35,85,70,72,138,79,155,82,83,87,49,127,180,88,18,39,193,119,214,232,177,23,66,104,177,10,98,32,50,30,176,220,34,82,167,34,25,65,47,108,62,71,26,98,46,74,47,96,101,46,195,246,53,89,115,64,157,119,97,123,85,145,59,108,189,112,120,55,31,29,186,220,349,189,83,101,118,123,125,39,100,224,112,199,71,96,40,9,205,157,157,58,124,40,117,9,227,59,246,32,122,29,79,233,106,85,170,22,78,205,39,56,53,93,135,77,79,177,171,68,52,65,269,38,160,9,49,93,118,68,66,140,74,45,128,96,81,20,160,140,172,95,223,80,14,80,135,19,148,35,113,86,153,46,132,130,165,111,91,176,132,81,202,169,119,38,129,108,167,77,74,30,162,17,142,76,142,54,229,160,149,13,66,142,84,47,74,60,39,49,211,115,94,99,125,84,143,129,153,130,83,183,163,99,142,62,84,32,165,244,101,207,219,66,84,176,49,69,140,105,61,78,25,92,205,75,64,165,34,180,22,112,148,54,154,154,37,117,201,136,179,65,129,49,164,79,96,74,60,160,19,87,51,280,183,88,110,163,89,112,39,250,122,20,105,176,323,159,12,124,84,139,34,104,65,172,40,74,120,28,38,51,112,70,161,56,94,60,96,56,59,49,29,149,34,101,54,55,199,295,50,87,102,116,110,144,153,117,149,105,78,59,127,19,122,228,20,59,151,160,211,73,259,84,116,211,134,137,94,203,36,56,102,158,43,55,83,93,200,24,127,130,116,91,105,128,41,20,24,30,66,214,49,40,98,93,105,67,134,96,109,13,147,119,86,20,126,137,91,139,252,91,45,8,209,92,51,129,67,38,31,169,607,37,114,89,101,30,60,73,50,85,45,153,164,151,171,164,67,39,200,57,133,152,313,107,114,138,21,59,43,82,165,112,55,210,14,100,136,71,13,61,25,37,47,200,120,48,10,64,87,134,44,119,96,119,22,18,53,38,137,52,53,52,62,106,51,106,181,47,130,139,102,87,16,31,97,403,195,109,128,147,160,109,103,38,108,75,46,24,133,52,27,26,27,58,158,61,47,57,181,56,60,126,27,64,66,76,70,92,66,175,27,134,74,98,323,158,88,165,63,13,67,21,52,130,97,164,151,38,46,67,84,49,89,54,32,91,162,174,152,118,40,198,121,93,309,100,216,43,37,153,205,139,768,16,81,173,158,62,32,72,122,165,67,64,23,114,182,18,103,85,122,128,118,45,38,65,154,143,164,24,171,62,64,78,180,114,80,266,136,88,82,74,39,176,49,52,102,148,77,73,15,21,132,132,106,103,185,203,75,109,92,42,109,125,65,94,49,158,19,41,91,31,39,201,79,19,51,130,140,100,46,68,19,53,82,74,108,60,23,225,102,33,104,34,77,130,62,202,214,87,97,78,80,141,30,71,142,126,256,112,77,45,112,104,104,105,101,76,99,54,108,55,19,102,110,127,37,55,119,97,77,121,96,18,149,48,39,35,99,262,57,58,257,90,47,39,48,234,133,152,80,139,71,38,136,170,48,113,72,126,200,118,61,152,152,168,64,179,28,114,95,28,225,171,202,114,92,140,190,47,178,119,106,118,478,55,180,24,15,170,108,196,98,130,109,510,62,154,134,40,27,105,64,37,125,86,217,172,139,26,73,76,21,145,82,40,49,208,62,149,126,174,184,62,130,64,65,27,114,121,75,183,104,164,65,56,200,67,55,26,151,67,92,141,54,97,131,95,24,109,155,109,112,84,55,21,48,332,13,40,125,101,191,80,106,62,199,18,13,17,62,218,126,55,144,68,137,167,200,106,50,215,27,30,25,166,193,100,15,120,165,73,91,109,90,54,35,332,34,93,94,185,130,214,172,186,140,228,145,24,98,81,245,42,131,97,64,115,97,231,73,124,122,36,137,65,61,12,236,125,70,124,53,111,116,32,106,17,217,186,251,149,86,97,35,58,164,26,45,164,152,345,60,64,46,190,40,145,136,135,40,395,178,73,101,178,75,19,105,48,130,35,77,189,74,137,151,54,238,79,162,122,212,115,76,188,44,32,35,387,185,141,160,140,114,116,36,12,46,141,130,85,104,52,52,49,122,26,67,14,80,174,208,123,161,156,59,46,142,22,199,63,22,88,77,133,90,208,146,138,10,163,39,47,176,120,125,88,257,256,124,13,57,57,191,66,166,41,65,88,124,21,207,220,178,61,84,33,94,76,274,37,40,69,44,56,85,46,99,149,78,13,39,344,84,103,101,168,95,103,312,157,14,35,72,73,161,215,19,127,12,180,79,115,168,134,185,197,109,102,205,97,213,244,112,96,195,155,241,91,63,117,70,99,56,130,232,125,62,128,48,94,52,108,23,137,42,27,143,153,177,32,252,42,90,93,61,143,139,102,55,145,237,104,31,185,121,65,42,84,98,106,20,130,121,192,47,82,122,49,90,124,124,61,79,31,164,107,63,103,41,180,75,56,99,127,125,65,51,26,101,227,231,8,99,55,119,141,37,215,108,156,40,169,240,111,204,24,111,114,50,38,103,129,118,96,40,37,229,163,124,28,455,399,16,139,218,55,81,229,66,52,73,237,82,76,222,106,144,123,99,50,209,254,72,65,41,35,168,131,57,143,323,9,155,47,39,169,36,93,84,176,83,141,63,149,43,58,50,203,64,114,139,18,262,136,178,33,39,65,153,151,204,143,128,91,176,111,57,179,30,115,124,75,156,160,141,183,81,74,57,162,115,267,53,151,159,37,39,128,111,44,113,223,199,120,97,88,175,108,219,142,256,288,62,128,266,77,110,120,214,32,216,171,26,134,110,26,24,93,58,169,85,108,167,42,126,90,156,48,27,160,31,71,93,153,43,103,67,32,152,42,135,95,80,74,53,35,147,72,37,156,194,88,66,100,81,65,115,36,58,55,127,58,75,52,138,167,68,129,29,13,281,77,117,140,146,182,82,52,191,55,93,85,122,17,137,80,64,46,218,52,101,134,57,151,178,24,72,60,46,25,37,69,97,163,44,143,90,145,98,63,98,83,19,157,98,51,90,49,164,58,63,197,22,84,29,51,36,154,10,123,112,127,168,134,33,121,149,41,312,105,65,31,116,52,58,175,34,34,72,122,196,72,73,120,87,69,65,94,85,38,33,129,273,117,88,155,55,243,85,182,127,250,135,216,192,72,34,175,34,121,67,83,42,35,54,270,61,241,202,82,134,138,219,82,77,126,39,50,94,187,217,63,31,52,44,217,210,50,65,22,254,130,356,127,23,142,173,214,184,66,52,36,118,76,77,50,29,195,31,208,42,74,136,19,54,44,152,105,75,289,60,115,119,32,104,135,182,46,37,28,118,72,23,67,178,93,93,41,152,69,51,91,53,115,55,46,66,108,189,135,19,39,156,171,243,60,137,58,46,227,178,165,51,38,50,70,190,14,161,29,124,75,17,27,229,34,134,59,45,202,41,115,183,173,136,57,76,56,172,187,86,66,117,154,60,85,26,28,109,30,112,210,53,73,241,97,48,52,45,227,32,62,119,178,86,105,50,144,35,145,104,38,46,157,38,67,26,148,74,163,56,162,132,185,282,235,68,80,201,61,55,86,60,133,211,47,100,112,42,84,228,81,54,63,119,122,42,90,10,89,122,87,169,201,129,141,45,81,59,148,72,232,148,160,22,232,76,112,105,38,102,58,59,149,62,40,127,37,52,78,26,135,91,121,44,201,93,345,80,175,287,328,37,155,59,342,144,110,163,194,18,159,202,179,53,77,72,186,69,38,170,97,308,139,165,105,104,102,145,100,15,94,118,164,65,192,37,81,99,116,116,96,156,115,157,106,88,165,88,134,30,77,89,51,62,122,213,237,109,113,85,34,78,39,42,83,115,197,65,199,140,202,29,39,144,162,81,106,35,211,16,113,120,95,131,72,68,84,193,66,23,123,89,39,80,86,158,84,52,181,266,56,133,67,158,24,169,120,68,32,311,81,59,32,235,211,13,170,768,110,130,27,13,171,216,38,48,83,23,165,10,17,71,95,174,118,55,155,13,95,12,65,117,58,68,53,113,43,99,190,49,155,94,166,83,84,13,23,53,146,187,43,111,125,122,55,56,134,115,74,104,100,53,121,134,107,88,60,72,67,122,136,67,121,46,74,96,124,40,181,254,26,61,40,106,62,37,103,136,81,62,146,183,37,122,153,197,120,71,30,106,69,181,105,104,133,124,106,79,32,268,33,167,129,46,65,145,29,62,39,61,51,62,102,99,97,281,170,91,128,120,108,96,191,22,133,66,78,114,46,121,100,92,187,51,188,129,169,45,23,53,60,54,28,16,56,46,50,53,22,36,130,137,29,116,118,35,68,47,213,16,189,82,26,44,92,45,130,214,129,143,149,103,55,230,145,70,211,127,142,73,78,56,208,91,208,11,127,72,25,154,128,126,48,160,313,157,75,151,93,55,15,77,233,351,68,73,51,126,162,80,123,35,191,118,128,57,128,252,281,134,154,275,73,204,36,117,91,101,124,50,240,89,65,59,78,77,114,120,132,215,158,198,80,232,161,48,111,165,63,15,135,198,161,211,125,53,117,115,47,163,31,23,261,85,165,25,41,79,79,17,146,26,38,21,192,74,132,23,107,146,189,55,99,35,165,94,48,193,148,40,23,97,118,59,50,49,67,120,162,77,65,126,140,192,27,50,32,53,96,267,37,59,35,89,32,32,36,134,120,70,130,64,170,57,32,94,71,150,65,145,144,126,59,126,138,123,43,64,26,114,203,77,139,101,104,20,22,22,145,48,91,40,26,210,95,173,73,227,36,196,101,116,87,45,150,89,44,114,85,42,217,177,157,35,138,146,170,93,76,87,170,33,137,702,153,166,76,41,72,88,47,88,21,25,84,176,159,77,224,177,119,49,32,117,232,20,37,36,111,187,181,81,86,48,210,40,177,35,141,196,83,68,155,49,111,14,25,133,120,189,58,35,87,45,90,183,328,57,169,107,134,161,24,125,44,173,105,157,57,184,207,67,85,69,58,39,111,140,151,84,129,259,16,76,281,76,99,86,173,29,88,111,158,29,77,61,145,55,88,65,94,83,40,176,130,94,165,164,102,259,181,137,86,43,121,137,170,91,13,114,190,263,88,24,88,54,98,136,58,170,193,37,41,115,273,158,127,125,42,40,200,22,215,10,20,81,82,40,145,36,43,16,215,138,182,99,34,54,49,15,37,169,163,37,110,134,181,62,134,69,193,146,186,130,67,51,130,197,144,100,78,22,163,131,45,7,54,54,74,152,66,105,47,24,105,47,145,65,119,119,105,147,21,45,41,83,150,30,85,143,182,123,49,60,20,17,119,116,54,100,140,55,62,118,60,104,125,967,146,147,247,117,424,84,61,147,157,84,164,100,150,18,36,55,104,153,144,154,176,97,167,50,95,22,109,34,50,130,214,100,87,82,113,267,68,156,159,84,57,172,191,107,61,62,203,130,27,67,117,90,136,42,50,54,397,185,129,273,24,229,14,51,98,104,73,45,150,87,135,147,51,128,94,43,109,65,34,174,69,68,90,69,149,63,107,54,218,263,51,102,134,105,43,152,90,281,120,141,51,52,20,121,176,159,96,92,80,46,175,109,61,91,150,45,29,164,144,47,205,63,117,104,99,26,212,40,89,141,111,124,68,45,118,70,142,60,93,172,130,122,152,32,105,56,227,157,44,86,152,20,34,47,155,70,37,17,75,37,96,66,108,117,101,58,133,95,82,130,30,607,60,93,47,43,21,196,69,187,14,64,72,335,37,93,171,205,104,148,138,92,81,104,62,195,113,23,129,111,76,76,58,94,65,129,81,42,229,133,91,216,182,87,27,80,159,147,246,128,112,70,42,67,62,131,113,177,59,147,29,159,69,49,81,116,69,129,109,65,131,63,163,212,203,51,198,120,27,110,150,97,78,101,135,91,112,38,149,71,65,22,120,51,71,59,175,200,116,48,76,241,99,124,34,100,112,27,166,157,27,51,122,127,53,145,141,42,130,178,97,20,120,31,177,56,70,73,187,34,145,134,26,133,84,104,61,92,68,191,44,100,69,106,84,115,134,57,147,65,36,68,38,155,190,47,134,65,82,119,132,191,42,34,143,52,31,11,145,105,116,95,20,66,35,40,22,197,51,51,60,51,159,152,72,39,68,266,67,55,51,266,135,85,110,217,198,57,42,90,73,122,103,153,134,186,119,139,178,203,94,120,32,97,146,134,125,63,79,290,218,217,85,97,87,56,149,251,48,34,138,154,72,52,124,126,254,29,134,173,93,130,211,42,71,29,132,129,85,94,48,33,128,22,170,116,63,65,150,124,28,77,70,211,60,146,139,149,72,51,39,66,61,63,41,105,18,23,38,135,128,143,53,53,35,94,21,96,124,66,95,105,69,62,252,22,81,153,51,111,104,173,104,101,45,38,12,140,98,301,93,90,63,124,64,88,51,85,64,287,110,62,113,187,122,143,120,150,115,93,147,271,99,47,49,131,100,61,347,33,139,117,72,130,149,158,75,55,33,233,191,19,94,271,186,172,87,77,97,12,218,140,164,154,7,99,42,96,23,61,24,26,155,127,108,99,78,207,83,157,73,75,141,206,65,155,131,70,76,130,155,92,58,43,193,216,115,30,77,23,16,145,49,59,157,108,105,30,155,75,72,139,133,104,126,17,47,67,69,19,199,192,144,30,46,149,152,178,51,67,176,235,257,116,14,85,75,198,214,76,65,198,226,107,134,87,27,76,196,114,68,50,182,117,184,130,27,134,182,178,220,102,139,97,140,59,52,80,224,88,108,64,60,64,79,165,134,22,7,180,26,126,90,62,96,323,63,146,45,52,44,39,37,196,140,181,55,175,200,157,20,43,134,53,172,118,58,403,91,148,35,72,57,50,106,80,151,207,112,27,100,19,180,76,21,52,173,67,210,87,73,133,253,38,51,92,72,110,74,66,157,24,112,100,60,41,136,27,199,69,66,16,83,32,260,139,19,22,80,112,209,161,59,61,73,40,100,98,14,124,36,49,62,127,171,97,172,33,70,9,64,38,123,85,391,108,143,93,175,39,189,221,42,164,50,122,138,87,160,114,32,185,178,53,105,40,111,66,119,52,47,83,198,200,101,12,95,188,23,56,192,71,86,85,120,60,75,74,113,87,45,49,37,45,55,103,161,216,105,196,65,17,124,20,33,134,158,121,82,420,117,22,202,217,19,22,33,228,76,166,101,132,123,32,78,108,142,135,97,88,152,46,236,18,106,114,112,47,173,39,106,23,60,124,69,77,169,137,119,167,111,34,62,37,387,166,155,37,70,144,22,83,107,113,86,42,38,100,44,56,87,85,150,47,38,139,90,45,97,117,192,138,98,127,174,112,21,131,67,215,129,187,139,78,57,147,136,36,51,97,107,124,94,61,179,207,86,66,34,96,68,144,84,111,26,38,49,35,173,144,210,55,181,191,81,87,47,70,54,33,71,30,27,159,190,118,232,27,148,56,106,87,131,172,29,158,56,99,77,53,39,61,66,119,72,55,78,35,125,46,172,171,93,26,162,128,153,146,29,179,26,10,21,104,69,53,91,12,210,31,19,31,118,152,80,86,200,66,184,105,133,45,104,118,207,74,93,171,65,98,54,193,143,85,26,85,105,249,148,146,65,176,76,122,161,118,21,175,104,49,40,26,255,193,260,89,150,52,152,50,208,36,105,198,132,55,132,40,62,82,41,82,77,20,40,241,45,69,41,132,167,62,38,61,97,126,156,176,175,118,30,226,48,94,104,49,124,137,20,87,207,86,86,236,167,193,61,138,132,74,109,80,308,21,42,56,79,43,116,71,140,12,81,215,28,85,42,201,74,142,43,88,34,44,40,202,166,83,232,43,64,191,117,64,276,201,102,60,122,258,151,274,107,32,58,285,234,58,88,181,121,116,227,139,38,148,118,194,200,63,62,22,24,151,45,84,130,112,22,76,110,36,24,121,91,147,76,35,34,21,303,79,46,91,66,172,55,25,153,57,74,55,104,54,118,31,60,145,112,496,110,161,37,155,144,109,248,34,53,106,146,41,40,99,38,31,130,34,39,115,154,137,114,46,133,88,260,50,26,62,176,23,88,96,181,119,36,101,166,250,18,220,15,57,83,57,208,11,89,70,200,55,35,9,50,26,111,152,20,24,42,167,122,32,111,162,143,391,114,193,155,159,109,33,29,76,403,158,85,15,106,85,174,98,196,174,155,41,54,127,62,122,21,43,83,194,106,122,72,134,129,60,15,227,122,15,84,64,91,510,67,57,145,108,61,225,176,129,155,13,181,42,134,114,82,40,134,106,12,179,62,148,207,49,82,42,95,97,143,47,68,92,130,48,45,300,144,28,33,195,15,173,100,40,122,207,31,17,88,65,58,155,173,266,163,104,49,133,52,106,68,82,81,82,43,118,71,40,152,14,67,64,28,57,82,215,59,147,79,98,100,71,18,37,144,37,44,156,56,76,46,19,156,101,216,217,153,111,146,51,274,17,100,218,71,215,77,28,65,113,70,52,256,66,29,203,106,27,37,102,38,145,218,62,60,246,86,130,110,15,93,137,46,53,115,49,204,135,18,28,13,77,23,93,33,182,120,78,170,51,194,66,29,62,72,85,64,25,66,89,146,176,208,98,30,49,150,72,82,174,40,32,85,148,13,73,269,21,156,49,42,112,47,104,95,93,46,134,26,172,183,153,23,23,158,36,237,85,68,307,201,164,15,70,214,43,214,85,142,177,138,78,208,46,152,31,27,116,104,118,56,125,158,19,196,114,105,52,106,62,60,60,83,82,170,266,216,58,15,32,24,29,70,32,120,112,118,168,58,13,62,108,50,171,107,14,116,86,137,64,56,46,140,134,124,160,145,99,161,93,174,72,206,46,102,131,190,70,136,32,18,210,164,82,63,146,82,98,272,65,148,153,131,126,96,88,46,146,102,48,67,96,77,108,193,198,197,116,195,35,45,112,98,78,42,176,110,77,83,13,15,46,82,90,89,193,73,144,392,40,125,153,177,162,114,266,101,96,172,219,97,145,21,81,40,45,237,146,225,158,46,101,13,102,83,83,184,111,50,73,48,104,200,165,71,55,72,56,139,175,118,110,184,133,67,45,49,18,96,162,28,43,127,42,48,110,84,163,161,128,261,28,150,22,89,76,150,228,80,72,58,38,100,58,14,54,196,200,80,30,34,155,104,146,120,109,52,10,192,36,64,164,218,111,67,100,105,212,72,45,38,48,154,35,282,52,52,111,109,33,140,108,178,105,112,31,249,30,177,59,27,22,144,40,134,77,123,110,220,19,24,141,112,81,199,49,94,167,19,129,165,48,110,86,28,123,89,186,182,115,94,67,68,18,63,40,120,129,196,12,139,156,24,47,103,104,163,62,87,144,134,83,122,154,86,133,44,354,175,98,9,103,148,159,60,108,39,75,181,182,50,78,78,134,139,122,20,133,154,121,23,127,185,81,47,78,51,53,63,89,105,95,150,203,178,20,69,178,87,27,224,22,76,192,47,117,38,271,166,102,220,26,237,80,233,185,104,34,122,159,196,66,127,24,44,72,127,132,10,90,118,88,132,176,36,68,158,136,149,170,66,87,173,145,83,23,137,34,81,180,141,51,81,84,99,42,113,47,291,18,114,94,179,164,109,30,101,51,26,86,154,160,26,51,100,131,92,188,250,132,27,71,72,67,45,145,45,17,92,65,117,47,63,100,110,250,79,120,282,167,68,892,119,23,31,28,55,155,139,19,106,69,177,296,114,167,420,139,126,19,99,69,9,177,33,121,49,95,60,63,79,115,120,76,166,220,101,24,47,55,125,95,80,169,71,117,52,248,97,23,127,117,115,63,35,91,175,64,151,184,114,172,58,113,84,73,241,142,89,51,181,90,120,141,247,140,69,64,54],\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Text  distribution in the context of \\\"premise\\\"\"},\"xaxis\":{\"title\":{\"text\":\"Len of text in premise\"}},\"yaxis\":{\"title\":{\"text\":\"Number of sentences\"}},\"bargap\":0.2,\"bargroupgap\":0.1},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('283cdc30-834c-4e70-be44-5b13b4a46546');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"code","source":"train['text_length'] = train['hypothesis'].apply(len)\n\nfig = go.Figure(data=[go.Histogram(x=train['text_length'], \n                                   nbinsx=50,\n                                   marker_color='skyblue')])\nfig.update_layout(title_text='Text  distribution in the context of \"Hypothesis\"', # title of plot\n                  xaxis_title_text='Len of text in hypothesis', # xaxis label\n                  yaxis_title_text='Number of sentences', # yaxis label\n                  bargap=0.2, # gap between bars of adjacent location coordinates\n                  bargroupgap=0.1) # gap between bars of the same location coordinates\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:14.823498Z","iopub.execute_input":"2023-08-09T20:20:14.823999Z","iopub.status.idle":"2023-08-09T20:20:14.856376Z","shell.execute_reply.started":"2023-08-09T20:20:14.823955Z","shell.execute_reply":"2023-08-09T20:20:14.855471Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/html":"<div>                            <div id=\"0b94cf84-2188-4335-a62e-6183c46e621a\" class=\"plotly-graph-div\" style=\"height:525px; width:100%;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"0b94cf84-2188-4335-a62e-6183c46e621a\")) {                    Plotly.newPlot(                        \"0b94cf84-2188-4335-a62e-6183c46e621a\",                        [{\"marker\":{\"color\":\"skyblue\"},\"nbinsx\":50,\"x\":[81,58,37,50,47,47,50,38,51,42,46,38,47,23,63,48,45,29,84,54,56,206,34,33,48,38,20,47,47,36,69,57,30,146,21,58,108,63,76,34,64,36,40,21,74,50,37,38,28,36,28,60,14,71,41,46,65,45,81,79,47,25,36,50,29,29,75,184,37,50,42,59,30,45,23,33,27,51,27,26,78,61,30,45,60,61,177,35,51,67,80,65,37,54,8,67,63,35,88,32,59,33,53,67,38,74,47,59,16,36,63,20,19,63,48,67,65,67,30,61,70,54,123,78,43,101,75,23,57,51,50,96,10,33,74,108,77,78,61,54,32,71,62,21,25,56,44,22,35,116,42,70,59,46,113,58,34,57,32,27,53,83,67,69,51,65,45,40,39,84,65,31,94,90,44,34,132,65,46,17,40,49,79,30,34,64,55,84,132,37,77,72,47,48,17,34,35,92,11,50,49,65,58,27,35,48,56,64,47,81,75,82,33,56,40,74,42,30,52,81,41,34,78,26,26,36,16,59,18,32,33,84,78,80,36,43,35,59,79,18,40,37,77,56,49,49,70,24,73,52,58,196,68,34,77,60,31,37,25,23,55,30,43,41,15,34,26,58,71,45,27,116,17,62,41,73,45,19,81,80,19,33,60,70,14,106,49,33,30,24,16,58,22,56,70,104,54,33,56,109,39,60,59,70,77,23,42,65,35,23,58,36,57,62,92,76,31,66,35,14,36,63,58,56,55,39,65,33,64,44,68,57,71,133,79,67,43,52,44,62,47,56,29,73,50,61,47,48,50,21,24,22,55,46,23,74,95,52,41,54,61,40,53,45,49,30,59,65,95,45,58,85,55,35,66,42,54,50,12,74,64,39,75,64,59,16,58,93,61,83,68,47,78,12,41,71,64,40,51,84,86,43,45,41,59,50,90,85,45,16,62,81,20,50,41,51,57,63,33,70,28,25,19,51,55,129,41,80,74,77,36,51,49,15,45,66,50,81,71,38,16,28,66,49,53,32,55,51,27,78,80,77,46,64,51,56,48,91,89,54,32,85,58,42,51,47,125,44,33,31,58,9,60,95,72,42,64,29,30,47,172,57,40,45,29,7,129,59,99,68,26,57,27,73,49,54,73,40,85,71,35,92,58,54,88,30,45,40,19,80,50,97,36,88,66,39,15,85,15,102,51,87,119,52,57,51,39,79,55,31,29,90,70,79,35,50,45,84,22,79,42,57,53,24,14,62,71,40,63,46,21,73,37,11,31,45,32,20,55,51,70,73,53,22,59,25,41,70,8,48,67,60,66,46,77,43,54,96,35,99,58,42,30,23,60,33,42,27,90,81,37,101,77,43,30,62,29,69,72,94,62,46,57,37,23,70,49,64,35,64,39,43,98,32,55,61,49,60,102,55,70,57,59,15,55,37,38,41,127,33,44,47,84,59,37,80,48,48,63,25,167,52,29,35,70,11,61,48,77,60,32,97,48,42,24,103,63,46,54,40,116,18,49,36,72,86,40,86,78,37,81,44,16,57,45,51,69,74,19,165,95,18,73,56,63,57,54,50,11,42,48,96,71,67,41,54,37,68,45,38,32,27,57,31,47,33,58,51,53,46,58,50,56,35,74,17,72,59,42,42,43,47,42,49,30,59,86,38,43,42,47,65,140,34,132,47,33,41,41,74,75,58,47,39,81,108,21,45,59,36,53,50,178,90,25,26,21,45,108,54,61,40,67,64,25,41,20,49,22,66,18,48,43,7,20,93,44,91,45,79,26,15,65,44,77,69,89,55,98,33,15,44,22,35,41,52,74,31,32,40,26,61,49,53,15,51,108,45,62,94,48,65,73,51,29,13,25,151,88,36,44,43,53,32,41,52,49,24,56,43,38,48,35,48,53,44,48,67,68,25,40,59,74,122,44,52,40,26,67,78,68,96,45,74,60,67,55,55,22,46,85,48,32,89,22,63,47,45,29,50,47,69,14,42,48,34,81,93,37,27,56,55,69,104,66,48,40,33,53,62,50,55,31,80,72,46,109,49,38,89,67,25,74,86,54,49,74,69,52,24,26,63,30,98,80,88,52,77,77,40,51,90,60,94,52,40,82,20,64,58,66,21,32,32,100,56,56,85,67,29,25,42,62,90,54,41,48,52,75,47,48,71,49,40,92,27,14,76,33,57,40,36,51,46,59,81,70,22,74,60,48,60,17,28,31,66,38,58,44,53,109,47,20,64,23,30,24,64,73,45,35,16,57,22,52,66,87,65,71,52,41,53,35,35,72,44,54,61,61,18,15,69,73,50,78,56,78,197,76,7,56,68,51,50,25,23,60,22,50,36,19,48,16,77,45,57,86,74,13,59,28,62,68,21,61,74,56,47,62,21,46,23,39,67,53,79,18,40,12,88,67,52,26,72,53,55,28,48,74,66,66,51,26,32,87,37,51,67,21,49,52,53,60,52,96,18,27,80,49,24,24,52,46,29,56,40,73,45,11,35,32,59,52,57,32,10,48,15,46,181,71,111,39,41,36,72,54,29,100,68,42,49,37,44,27,43,69,62,42,129,52,53,18,26,56,104,44,73,204,40,41,30,57,74,80,37,28,30,12,72,11,164,25,67,34,44,33,67,30,27,31,84,32,77,35,45,33,37,44,109,36,77,63,39,47,66,50,48,34,33,118,43,97,60,27,143,31,56,29,122,51,129,41,37,22,93,36,46,55,32,39,40,29,43,30,12,61,72,43,105,62,13,71,90,46,60,57,77,110,67,49,83,38,80,53,43,61,76,56,46,51,47,34,54,47,54,54,72,65,55,35,24,36,22,42,68,41,135,54,66,54,75,66,28,34,62,26,33,42,35,67,36,60,51,56,41,48,120,40,36,61,52,48,63,51,69,52,53,27,91,92,48,59,77,26,56,34,40,40,35,45,86,22,47,87,55,25,47,75,68,47,92,54,42,25,22,34,37,79,77,56,72,40,49,99,40,70,56,47,79,85,85,52,32,44,42,16,20,74,46,34,76,30,84,27,35,53,28,46,50,36,86,53,28,54,64,50,37,78,110,72,50,60,67,64,50,30,94,51,41,71,23,50,19,126,47,152,24,56,78,62,13,65,53,44,81,52,18,51,42,49,67,30,58,64,72,14,46,102,46,28,69,56,55,44,102,83,48,48,14,40,63,81,39,41,52,41,40,46,51,16,46,36,13,38,74,35,44,45,35,17,43,58,44,56,25,81,32,60,61,101,19,40,53,50,27,57,36,23,32,90,33,64,42,76,55,105,86,10,55,97,44,48,46,46,54,60,11,50,38,50,62,62,69,55,103,75,44,72,55,48,44,23,77,28,63,49,103,40,58,85,46,56,39,30,29,52,66,65,73,31,71,77,70,52,49,45,52,49,14,36,61,27,47,36,66,112,49,41,57,52,23,65,76,72,49,11,75,50,51,29,57,12,39,43,15,22,23,51,73,26,43,86,75,48,24,13,54,57,34,49,40,17,52,45,68,67,60,66,55,65,58,49,21,58,25,53,69,31,72,71,64,47,45,74,76,107,63,39,118,30,52,27,44,45,28,74,55,61,57,57,54,54,52,36,57,49,89,21,89,42,82,9,79,36,55,42,31,52,68,24,28,49,83,73,65,60,35,25,33,39,17,29,34,26,44,42,40,42,84,104,58,72,61,61,36,56,44,55,71,65,48,68,29,78,63,57,47,44,148,72,86,38,51,49,47,28,63,46,83,32,37,41,53,67,127,30,36,103,47,87,98,57,37,32,51,52,33,50,52,67,94,67,35,85,56,66,25,56,84,72,42,51,31,103,60,68,69,50,24,42,55,52,41,62,55,24,46,28,18,54,136,20,23,82,52,54,46,15,25,78,23,97,52,55,44,54,27,62,75,47,53,76,84,19,53,105,26,39,34,42,112,10,105,56,45,56,32,38,55,74,34,49,48,74,35,12,59,55,47,39,67,35,34,40,50,53,44,101,28,29,26,66,41,57,89,74,105,37,29,93,26,74,38,34,19,101,23,60,29,63,21,52,36,17,26,25,60,40,20,36,60,36,82,51,67,67,39,54,60,55,67,48,35,51,23,62,99,77,95,22,19,52,57,69,20,32,35,49,71,30,10,53,75,41,105,35,35,21,48,78,52,78,74,66,36,35,38,56,46,53,67,37,29,42,32,32,84,52,65,99,72,46,35,77,81,29,78,74,62,68,80,123,18,22,37,59,87,34,48,99,62,49,54,46,16,38,52,54,52,11,98,48,77,26,48,99,74,22,39,11,9,30,54,53,43,60,117,13,56,40,104,73,63,51,49,50,30,34,73,26,14,75,59,40,76,39,46,43,6,66,48,20,84,57,51,56,53,34,22,36,80,70,101,88,82,71,87,132,52,76,58,54,46,94,84,45,25,58,61,62,93,38,67,82,68,70,49,72,49,69,45,40,63,21,54,21,71,41,70,28,71,56,51,73,137,55,63,62,50,54,75,38,60,43,50,55,56,40,40,44,64,42,55,30,47,60,89,89,56,48,22,14,25,20,40,34,98,130,70,74,98,8,11,56,78,118,38,36,20,80,49,103,33,34,30,56,90,90,87,36,50,50,56,54,54,37,37,14,44,63,51,34,48,56,64,41,111,98,50,54,75,118,67,27,59,37,49,43,78,43,129,67,70,67,75,88,28,50,44,62,30,70,44,98,42,51,90,16,25,48,45,50,48,39,110,99,56,48,56,26,45,78,105,39,101,78,75,88,63,44,42,49,44,11,49,57,28,75,37,16,40,32,17,60,46,66,25,64,34,20,43,81,76,53,57,30,94,25,32,63,45,28,43,50,36,25,49,42,45,60,36,51,23,115,75,89,39,55,60,63,64,30,41,34,38,74,115,85,276,26,39,72,59,42,80,97,44,33,72,12,14,82,14,31,99,76,87,32,22,64,60,36,68,34,45,38,46,15,79,34,95,34,41,55,59,54,71,84,37,38,47,83,64,40,63,76,44,154,33,53,41,81,91,45,82,39,55,51,55,88,49,71,34,35,36,64,35,70,58,28,46,58,32,62,53,39,43,43,68,68,34,46,80,75,56,29,88,72,107,63,54,55,31,41,40,36,43,34,73,57,34,113,62,73,34,55,30,30,45,28,63,35,30,50,43,84,74,33,18,90,28,54,125,61,68,22,40,53,49,28,97,21,71,59,45,60,17,37,39,36,39,198,36,68,13,57,24,34,70,74,37,40,47,35,57,125,22,37,41,28,37,78,66,22,60,46,47,66,79,45,43,37,60,55,51,65,90,29,74,52,66,61,33,32,91,35,66,29,67,33,60,103,39,39,25,51,100,52,34,59,36,49,10,35,44,60,54,95,41,83,43,45,36,59,55,47,37,59,50,42,70,36,73,39,86,36,77,57,74,46,82,59,26,73,32,71,43,83,10,47,81,54,37,43,24,74,94,46,51,38,67,51,38,42,55,35,37,79,28,63,34,60,35,46,46,63,25,47,42,39,32,57,62,86,36,51,27,33,77,26,78,8,47,31,55,44,24,69,53,11,26,113,58,35,111,49,35,78,8,50,55,82,30,29,55,73,48,48,72,50,62,63,13,54,71,47,33,15,96,48,56,31,16,28,44,63,44,55,16,48,57,21,65,71,52,61,26,46,72,109,51,77,48,21,72,70,52,63,30,43,35,42,46,57,36,48,58,43,77,59,98,28,71,57,86,49,18,66,44,71,47,54,48,59,77,49,46,46,48,79,9,37,104,74,17,54,43,66,72,84,106,48,35,50,22,41,79,15,68,103,96,28,102,61,123,70,41,28,61,94,99,61,52,67,63,59,38,77,47,105,43,85,51,49,63,52,32,44,43,30,34,66,52,42,68,71,92,38,30,71,45,168,47,115,42,59,38,57,39,53,96,108,35,66,42,99,51,31,50,90,58,52,116,72,50,63,74,68,29,82,47,91,42,55,30,46,53,141,59,45,61,33,109,33,55,59,40,29,46,35,33,50,43,40,161,49,44,98,39,40,59,32,25,64,87,64,52,105,58,13,85,76,105,75,42,55,54,49,35,53,42,75,60,67,35,55,77,27,51,45,26,119,30,99,35,36,20,27,66,41,100,54,19,90,109,98,48,64,99,48,29,59,71,48,47,41,42,31,49,26,78,70,61,56,44,58,55,66,60,46,89,33,57,71,128,67,53,44,81,34,102,34,42,18,60,64,14,45,38,55,27,50,13,51,19,41,65,77,85,112,27,80,26,50,40,31,40,43,45,68,72,23,35,32,114,24,52,64,85,59,84,62,50,58,33,44,96,42,57,91,48,77,97,125,60,59,28,77,67,46,47,32,31,51,51,46,67,72,51,28,49,36,111,91,16,82,90,39,84,13,89,137,55,45,16,39,76,41,53,48,50,18,65,41,45,88,41,57,15,61,23,29,76,57,36,47,54,98,28,69,44,40,16,34,32,38,42,51,59,50,47,118,33,100,68,51,27,67,78,57,65,50,78,96,71,84,67,131,9,51,39,73,23,58,107,49,65,73,84,15,32,42,24,72,58,37,140,84,32,56,43,60,34,42,62,41,62,45,35,25,62,49,27,69,21,49,70,41,70,73,65,43,73,64,69,45,37,67,67,94,75,24,35,82,64,21,79,52,51,63,87,65,59,25,72,60,22,69,33,69,56,68,51,22,73,73,68,58,43,34,13,13,69,31,26,47,30,67,58,62,50,54,80,23,34,27,34,48,56,74,46,61,56,102,48,22,68,62,44,44,88,38,37,16,67,28,50,29,29,50,50,65,35,67,25,116,86,48,63,32,36,61,55,37,70,76,65,70,46,119,22,57,29,32,67,73,56,38,53,73,50,103,57,81,26,90,62,68,49,53,66,61,51,16,32,47,66,111,45,29,41,27,89,48,71,57,59,82,70,42,41,84,106,97,45,82,94,61,63,61,79,67,41,50,31,35,83,24,31,57,68,53,59,130,73,59,60,80,96,130,56,78,94,65,42,68,43,61,56,32,57,88,15,103,86,20,49,109,90,96,56,27,98,56,68,72,54,48,49,42,22,35,85,82,46,29,53,17,89,57,43,46,68,71,40,69,43,49,24,38,43,52,22,56,49,30,29,48,68,65,44,46,46,66,104,64,65,36,25,34,27,46,96,83,14,73,56,57,15,73,70,52,75,53,29,95,42,131,35,59,43,30,53,85,52,70,74,45,11,62,61,10,48,44,45,25,57,48,85,56,39,40,58,77,21,72,21,11,58,71,47,60,50,32,44,54,50,76,56,90,66,36,54,24,30,49,38,76,88,52,70,62,65,52,24,17,43,59,67,61,63,26,73,30,101,85,43,83,66,53,61,41,69,22,54,57,23,66,57,36,69,70,65,22,35,135,68,33,30,118,71,53,26,122,43,65,50,87,22,39,50,72,61,50,8,60,38,46,35,36,44,50,74,75,58,70,110,26,42,13,57,61,13,94,31,54,31,44,36,90,42,74,81,30,7,87,34,39,49,73,36,88,21,41,43,37,50,72,48,68,53,71,88,111,42,40,61,42,14,64,43,36,42,14,57,11,63,47,47,8,20,37,37,23,42,57,84,60,90,51,33,37,21,47,57,58,38,65,70,15,63,54,66,74,44,36,36,71,101,88,112,21,12,17,61,60,50,58,12,48,13,76,36,20,27,65,30,41,54,69,64,16,43,49,37,47,119,27,70,90,49,22,103,34,43,28,64,72,52,44,28,63,69,73,87,114,93,49,93,31,58,45,41,28,76,85,83,48,19,52,53,43,10,48,27,40,56,58,99,50,51,83,71,93,33,47,40,46,45,45,41,57,48,15,22,49,32,46,34,86,123,70,36,75,59,24,30,79,56,24,54,84,66,54,79,80,25,92,81,98,52,47,74,81,39,50,24,76,33,45,49,68,75,39,51,42,107,39,58,62,69,45,36,25,65,67,20,78,68,66,25,39,50,102,60,51,59,77,57,62,58,58,67,23,41,72,45,46,78,22,29,81,53,31,63,63,31,57,79,44,99,52,38,24,43,29,106,34,39,114,67,21,49,44,53,29,32,77,63,166,59,99,59,12,15,65,46,18,35,49,76,49,143,44,40,13,31,36,46,34,46,38,93,26,16,87,51,19,39,38,44,61,58,55,51,57,95,9,49,31,48,98,42,90,87,32,71,101,60,61,80,63,27,84,83,58,21,63,53,44,38,39,22,55,53,50,41,34,57,89,71,26,107,46,40,66,54,68,89,31,25,43,112,103,30,32,91,49,46,81,23,43,49,41,93,59,94,76,45,96,52,63,126,67,84,60,58,58,47,66,30,41,69,122,22,47,28,87,60,81,52,71,78,66,60,15,58,75,153,90,16,51,20,23,56,56,84,102,36,56,14,18,38,84,38,34,22,72,42,22,80,84,71,81,60,19,35,78,45,22,122,53,67,64,29,119,49,27,51,47,50,55,50,51,32,46,33,29,93,32,53,77,65,48,29,66,74,43,42,63,122,46,112,40,121,35,20,79,42,52,111,34,28,60,35,48,88,45,48,65,14,8,85,78,28,53,137,63,76,29,12,59,82,55,75,69,68,76,37,71,50,49,56,44,40,29,89,16,63,44,63,39,41,47,48,65,62,32,95,68,66,124,69,51,60,72,84,42,58,46,34,40,35,44,52,21,74,41,56,60,37,35,55,82,43,73,44,48,57,125,52,117,38,47,33,39,35,62,98,47,46,79,65,82,32,39,133,37,57,59,69,68,90,60,34,23,55,66,28,35,79,17,16,90,44,50,123,62,54,67,43,51,24,38,52,49,69,66,67,38,52,165,37,48,46,29,69,57,34,67,32,50,69,110,23,47,60,66,67,110,29,42,38,93,80,76,34,41,96,97,24,39,64,78,53,46,23,60,94,137,80,49,60,29,53,31,64,26,54,169,64,29,31,62,55,54,59,67,82,51,65,31,91,59,20,20,99,74,11,52,61,96,35,61,36,58,46,84,44,23,54,63,47,79,62,56,93,55,38,31,51,77,49,21,65,8,56,89,17,94,89,69,42,25,40,60,75,61,76,68,45,76,85,73,20,53,79,30,66,50,98,42,62,59,16,118,31,24,59,31,64,55,33,39,98,36,25,45,28,100,57,40,66,52,28,104,9,18,69,41,22,91,96,46,43,55,53,46,72,27,30,58,61,80,47,57,96,45,28,29,25,38,60,26,50,113,67,50,36,63,28,43,87,73,16,139,30,55,40,58,71,58,66,44,72,61,31,77,93,64,28,77,57,76,67,9,35,25,40,57,64,96,66,53,101,87,61,64,56,40,48,76,34,58,64,84,54,18,32,31,38,47,49,60,25,91,47,64,98,35,97,61,37,68,47,49,74,19,29,54,65,46,36,24,100,68,58,25,19,50,76,23,70,60,17,81,73,67,49,74,76,71,27,82,76,77,19,82,75,59,37,63,58,88,25,71,45,51,50,37,75,34,62,73,76,74,47,57,44,81,34,29,34,39,77,31,44,13,58,77,135,44,29,109,74,81,12,29,23,38,58,146,49,40,33,40,25,54,42,92,35,52,31,38,54,25,35,35,50,110,22,69,96,39,89,30,28,55,44,16,18,146,32,41,38,32,38,58,54,47,53,54,56,49,61,44,46,84,96,45,76,53,34,55,59,71,42,52,85,57,63,58,116,40,64,52,46,31,53,28,38,40,24,56,18,60,95,53,82,44,28,39,68,47,48,35,47,50,58,8,63,17,61,83,58,82,22,90,71,32,51,22,62,43,99,44,38,66,43,93,64,47,70,32,65,8,77,43,106,34,29,57,49,126,56,26,9,38,59,59,53,72,35,56,46,64,70,42,42,65,45,15,63,70,64,41,105,59,70,38,31,49,32,28,63,34,83,64,72,100,69,45,27,32,91,53,36,43,102,49,50,126,25,71,53,91,12,56,78,60,66,35,91,69,18,14,56,35,51,60,15,54,71,48,50,90,55,48,33,24,19,48,65,79,55,13,48,45,93,18,22,121,23,79,94,52,18,71,68,89,42,61,106,119,39,25,43,54,58,42,16,59,48,53,90,58,33,19,31,36,24,66,95,43,87,53,14,44,140,51,67,60,68,49,25,68,44,53,57,85,37,55,23,29,88,91,104,78,62,21,81,88,37,62,55,87,37,51,16,14,47,66,67,18,48,78,13,103,27,95,52,40,19,97,23,42,9,38,35,120,54,34,33,53,39,53,43,59,24,64,64,53,30,26,46,50,47,79,27,99,40,52,34,68,44,32,86,29,34,48,67,37,62,37,42,90,32,36,31,58,47,39,89,17,58,24,43,40,82,127,54,39,55,105,131,24,65,33,57,55,24,89,44,79,32,49,66,57,38,65,46,30,84,46,35,37,66,23,151,11,36,59,37,94,35,12,59,46,42,54,15,40,33,82,51,27,70,31,45,48,90,101,13,43,61,18,61,35,29,12,36,48,54,15,50,63,46,6,46,18,55,81,42,45,91,49,8,37,43,76,55,53,53,9,41,94,57,78,24,61,70,70,55,33,133,99,19,37,53,19,45,70,30,40,45,56,48,82,61,55,127,18,77,79,79,64,36,44,34,23,90,62,72,24,47,64,40,34,56,58,69,46,25,16,45,41,112,25,27,54,32,101,33,94,79,38,75,29,55,61,44,80,99,27,42,48,84,33,23,49,12,88,56,56,25,39,50,31,36,24,35,43,82,80,54,75,104,59,64,13,29,64,29,41,82,65,70,5,51,25,50,14,53,40,48,72,59,27,18,65,38,55,72,22,85,77,80,68,108,73,48,52,52,42,49,9,60,58,41,51,100,75,19,43,56,33,62,55,70,23,57,72,46,23,34,80,55,66,54,47,21,104,70,62,32,80,50,101,81,16,64,99,51,26,52,31,69,124,113,65,48,53,128,15,104,39,49,22,93,92,58,43,82,84,55,38,68,97,72,86,85,33,50,6,23,27,41,55,18,116,52,69,39,49,45,48,89,70,40,65,46,26,57,57,62,30,99,73,39,208,40,67,37,59,57,55,41,45,36,40,21,54,14,49,35,35,10,55,102,51,59,22,151,49,65,54,52,95,50,65,36,40,61,38,87,34,51,56,46,48,49,19,33,65,54,12,44,48,82,32,68,61,30,129,81,33,30,46,55,52,80,79,88,39,50,15,71,74,48,85,43,105,53,85,71,68,33,49,14,82,11,71,46,97,84,23,40,63,59,13,72,57,44,28,24,37,55,119,48,67,49,41,68,87,73,41,45,67,65,53,63,54,11,61,22,62,54,86,124,48,97,102,24,95,48,47,103,63,60,84,30,34,53,25,21,62,53,37,63,36,34,49,31,32,54,9,23,105,26,82,34,82,77,47,39,44,16,56,59,62,34,51,49,29,26,90,64,94,39,39,127,51,43,40,39,87,71,102,71,42,66,47,112,72,32,43,23,23,47,23,29,52,40,95,48,20,41,84,50,66,90,96,60,40,39,58,59,83,81,24,8,18,91,32,50,45,10,54,33,62,43,62,48,54,36,47,67,47,43,37,35,32,69,44,40,21,23,80,66,52,30,37,56,74,83,79,25,56,70,86,45,52,86,70,91,25,53,26,42,11,41,38,44,44,71,25,55,59,133,77,42,96,110,56,25,49,41,62,25,33,117,74,56,69,9,99,34,33,65,34,68,42,57,86,88,42,45,45,34,65,65,31,103,44,56,125,67,29,20,19,58,63,48,36,106,61,46,32,20,32,57,44,57,72,84,78,30,58,74,42,46,33,63,42,71,35,27,70,76,46,60,62,18,101,36,46,81,75,45,45,35,71,70,36,33,13,76,44,18,35,62,41,37,59,52,59,46,69,58,30,45,49,50,43,65,65,77,48,28,33,104,40,31,71,44,53,46,55,67,70,68,86,64,76,27,29,63,22,73,29,78,90,49,39,30,46,34,36,21,38,28,39,40,52,33,10,47,44,114,58,42,49,81,94,60,81,92,58,23,21,76,34,60,45,52,66,20,39,52,102,65,60,50,77,46,22,58,93,47,47,11,10,74,40,47,38,51,54,27,82,57,68,97,27,35,87,19,45,42,51,24,76,67,58,45,57,31,43,76,73,35,46,89,74,22,15,41,52,39,59,13,87,51,37,199,33,71,50,28,56,34,26,44,79,27,62,33,32,123,21,32,164,33,105,46,57,32,41,77,65,37,48,38,60,44,32,31,51,23,54,32,104,64,79,53,53,90,100,68,74,55,62,108,35,74,44,69,42,34,133,15,32,27,15,60,37,90,47,49,60,22,31,46,41,58,38,21,27,145,35,34,26,48,101,52,23,64,72,63,15,51,38,22,47,73,84,41,38,40,44,64,51,159,25,93,54,61,52,22,81,18,40,28,64,85,63,36,101,79,47,22,81,58,28,56,53,35,61,70,11,45,88,32,45,25,79,34,43,22,26,62,27,72,45,34,33,44,96,111,57,92,83,54,108,54,87,63,25,97,41,28,43,49,37,85,44,44,59,48,54,32,27,57,23,8,35,55,32,61,77,79,58,11,90,71,59,37,38,54,133,89,37,31,15,43,82,27,40,62,43,89,28,61,53,35,23,64,42,49,20,59,53,27,62,31,37,36,73,72,75,64,66,17,73,82,25,37,64,42,74,40,48,44,48,41,50,130,33,65,14,40,46,31,35,126,63,37,39,54,50,23,33,58,44,39,31,38,16,18,45,62,75,66,38,33,44,37,33,57,69,153,107,43,79,14,37,22,55,31,31,40,57,78,78,38,30,52,71,191,66,52,21,27,58,63,44,31,31,33,66,71,39,36,33,75,34,39,87,53,74,37,30,76,55,75,94,48,69,52,32,13,61,38,73,52,63,64,8,82,32,101,42,12,8,37,28,26,75,35,43,17,19,71,22,53,17,31,28,64,46,28,31,56,34,65,47,59,46,48,51,51,69,36,37,55,40,117,48,25,52,47,41,24,71,42,50,58,63,20,13,57,48,64,47,93,116,39,32,84,30,75,58,45,22,87,63,41,54,57,58,55,42,38,59,131,49,35,37,24,60,56,34,62,78,43,136,52,39,30,43,62,34,30,65,44,31,71,23,18,20,29,41,62,24,70,45,67,36,72,45,67,27,14,46,49,109,77,13,36,31,60,23,75,37,18,81,78,107,70,62,67,41,32,40,74,61,82,17,80,56,62,71,63,63,59,9,37,29,16,88,60,83,28,42,36,55,64,80,20,80,25,64,88,58,31,89,40,59,48,23,63,61,60,72,49,110,83,27,44,84,61,41,59,63,91,68,32,73,75,53,16,25,67,44,57,16,114,27,51,57,20,68,32,34,59,59,79,55,39,86,36,71,62,100,68,33,41,52,35,65,46,54,62,84,43,48,63,70,45,40,40,31,40,42,30,69,34,70,40,34,49,33,59,28,49,125,49,47,68,46,65,50,71,47,78,57,53,71,29,60,42,39,60,76,25,89,33,25,116,49,51,68,29,15,89,79,32,54,73,71,33,26,51,39,50,48,50,52,32,66,54,47,24,28,52,54,61,50,57,56,39,78,88,55,44,42,36,40,20,80,88,68,31,74,66,66,63,13,29,63,45,94,44,69,45,51,31,40,36,21,48,24,72,61,117,62,47,32,38,17,177,36,69,34,45,41,64,61,28,66,24,64,40,37,60,46,71,44,52,62,61,77,31,29,57,34,25,34,36,49,53,41,48,52,62,52,34,110,44,11,16,35,62,56,57,50,34,50,68,55,28,146,66,39,32,70,91,35,16,52,63,46,30,15,99,49,134,52,115,25,47,33,90,32,28,83,93,21,56,41,33,55,66,44,38,73,25,68,107,29,40,44,35,69,14,49,36,45,55,15,79,85,32,70,122,21,34,52,53,11,27,53,41,42,69,27,25,61,54,66,99,58,54,32,24,44,41,49,80,34,40,65,50,41,43,33,71,58,17,47,62,21,51,67,36,35,18,80,50,84,59,52,87,45,75,37,57,79,78,37,40,63,71,44,25,52,5,40,58,50,40,56,59,52,60,28,52,33,21,44,21,53,74,75,35,41,31,58,69,36,67,62,76,78,33,64,72,44,40,17,118,59,53,36,40,105,41,10,65,78,63,84,33,40,111,15,51,88,36,47,81,60,29,35,28,57,10,42,53,41,30,69,61,23,62,30,106,14,56,14,26,37,100,29,66,36,26,55,82,48,30,73,67,42,110,31,56,29,56,37,32,72,46,34,97,56,75,61,46,22,62,56,35,24,34,45,32,65,84,133,39,37,52,87,43,25,69,74,47,78,62,49,27,53,81,44,50,28,55,54,56,42,34,37,72,149,51,34,102,81,18,84,11,39,54,26,84,44,55,31,65,95,111,76,38,39,60,30,50,108,25,56,29,23,51,87,67,53,78,47,86,37,43,58,79,48,26,42,41,74,18,24,14,52,52,37,103,35,32,78,28,115,68,20,43,70,48,25,111,58,20,50,34,78,27,44,51,11,80,53,27,69,74,24,35,25,134,61,69,89,41,50,66,42,75,66,22,88,56,48,48,30,53,68,124,40,41,59,94,18,48,69,28,52,12,34,43,59,53,31,6,56,32,39,87,42,100,58,14,32,42,82,72,32,96,26,28,46,79,57,42,35,38,110,40,35,31,70,46,78,43,68,46,64,21,32,45,56,15,63,54,28,71,35,34,71,82,55,10,40,58,85,34,52,59,35,68,94,17,92,77,51,23,44,70,59,145,40,12,77,31,57,104,51,34,77,26,75,46,36,38,38,71,109,62,63,106,40,34,53,88,53,57,53,40,54,13,61,53,52,29,40,65,67,46,58,68,57,93,47,53,56,75,43,58,22,60,41,55,33,65,39,42,32,45,44,57,17,46,32,13,18,94,27,55,29,59,39,41,97,35,35,24,54,76,37,64,35,67,72,153,44,51,54,89,24,51,59,67,40,42,63,86,14,14,24,79,26,67,66,100,58,85,76,36,49,66,25,53,46,52,27,39,121,56,77,19,58,78,25,54,73,73,57,66,126,54,35,44,53,56,56,40,67,66,24,92,73,36,59,76,23,36,35,62,69,35,96,145,45,59,39,22,16,59,28,73,103,43,46,49,54,82,50,25,51,54,35,42,19,49,27,21,61,134,47,34,39,43,80,48,68,38,52,10,46,47,42,47,39,42,24,64,71,73,54,78,29,31,30,36,30,23,14,66,51,39,37,100,107,20,49,55,52,58,82,70,73,72,35,57,29,44,37,42,27,26,49,55,103,68,27,60,19,73,49,56,54,40,32,31,14,51,62,77,42,59,47,32,26,42,40,86,60,90,28,39,31,116,42,54,119,47,47,28,71,70,34,61,68,54,44,71,56,59,64,29,54,159,34,83,20,62,37,17,46,34,34,77,70,69,67,85,47,35,50,61,99,88,81,23,102,52,40,64,41,48,32,69,149,72,27,57,32,30,65,65,43,44,54,85,35,42,59,21,64,37,78,54,84,58,29,101,54,51,45,143,37,36,33,60,24,56,27,68,77,34,63,112,69,56,28,41,56,108,19,62,42,24,51,41,61,25,36,72,9,55,72,63,34,16,96,35,63,26,55,85,53,42,92,54,32,82,106,53,58,108,56,44,74,46,51,35,56,39,67,85,23,52,22,61,90,76,47,81,49,58,30,26,68,57,61,61,39,59,65,56,58,82,62,78,70,36,19,49,48,84,37,48,28,42,95,42,84,84,52,32,61,20,93,38,49,78,25,26,13,92,71,59,60,93,64,52,34,47,60,94,86,26,65,103,77,78,116,9,33,72,59,21,56,40,83,78,79,31,76,35,25,54,53,65,52,40,40,48,82,23,41,71,55,29,23,67,45,24,33,68,61,54,66,104,56,88,71,90,56,23,123,51,52,84,56,74,104,34,74,48,37,48,137,79,50,28,36,51,34,81,9,18,52,44,90,62,54,28,91,85,47,37,47,34,49,24,47,80,29,87,56,24,57,53,31,48,68,62,55,109,28,51,42,73,29,37,34,104,83,89,71,59,58,144,55,60,76,77,49,56,37,43,85,46,39,56,103,46,37,50,56,51,77,42,69,43,82,64,59,70,118,59,64,25,73,18,45,44,86,45,88,16,38,15,28,35,40,76,30,97,70,38,45,29,84,83,72,71,48,55,24,84,59,24,66,28,25,58,54,15,53,52,60,37,32,105,55,78,69,64,71,39,83,27,19,44,91,68,64,86,26,38,80,93,32,39,35,16,90,82,82,23,26,18,67,20,58,33,58,62,75,60,42,59,84,62,81,16,65,74,39,92,40,68,58,66,42,62,37,80,10,82,44,73,44,44,29,15,28,53,30,67,28,72,103,49,26,59,56,82,56,42,29,30,80,77,56,101,50,63,80,40,129,20,39,102,23,58,95,75,119,108,35,48,95,63,100,32,69,29,27,45,28,54,54,31,58,42,55,23,40,53,52,52,52,62,67,40,31,29,90,91,56,54,50,69,98,16,54,39,28,84,10,79,14,87,22,103,67,39,29,99,27,49,42,34,55,71,36,25,18,49,38,31,48,42,11,79,42,69,32,36,72,70,66,97,29,78,38,98,21,48,71,56,111,20,34,79,23,37,90,53,40,58,37,43,62,55,45,35,31,27,47,56,75,56,24,34,102,87,62,105,64,37,31,49,34,54,67,71,50,48,44,57,48,22,52,55,40,33,40,34,54,24,46,145,22,48,28,16,54,38,38,86,42,89,79,29,26,74,51,56,23,57,40,29,75,58,55,59,54,56,75,32,30,51,25,113,32,58,60,59,55,63,36,42,18,8,9,10,29,68,33,83,26,57,67,84,48,82,66,23,133,24,47,36,72,37,64,54,47,87,56,50,50,12,51,43,138,62,72,63,74,55,38,20,67,65,56,103,25,18,32,48,47,20,54,86,39,36,60,37,71,37,28,28,55,88,95,99,31,53,27,74,45,42,42,95,27,82,39,76,47,37,43,57,82,36,28,57,41,45,50,54,80,91,87,36,82,77,43,31,72,47,12,10,71,39,48,29,65,24,26,58,21,43,60,37,114,86,64,19,50,41,16,63,26,29,98,56,34,79,49,44,115,34,69,58,35,32,90,14,80,80,59,80,32,135,171,61,32,11,53,30,59,100,27,23,55,28,101,62,38,54,70,57,39,67,59,59,41,37,78,41,29,35,45,35,53,45,80,123,43,48,47,30,91,48,51,91,50,38,119,40,48,17,45,74,101,55,156,57,118,82,28,45,45,33,104,22,76,55,31,71,28,60,99,58,46,31,53,29,44,49,30,13,43,20,47,66,33,27,56,61,89,94,89,88,89,73,29,43,32,27,87,70,46,84,101,58,129,40,36,37,36,27,75,32,35,47,26,57,55,78,67,51,49,58,54,44,29,82,43,50,33,38,147,33,142,43,90,43,35,72,75,11,89,27,34,47,89,63,46,63,81,39,28,41,42,46,33,48,39,93,26,47,15,75,74,36,48,50,24,105,38,65,56,93,54,68,63,32,33,98,74,38,56,59,29,52,69,46,69,50,89,60,41,66,114,22,88,131,58,49,14,90,45,24,48,59,74,55,33,38,70,35,74,24,31,13,47,24,62,95,48,38,45,23,52,53,21,56,30,40,50,38,60,43,101,14,56,51,55,58,47,14,43,43,61,50,52,110,65,71,84,25,58,22,59,58,63,63,53,87,14,53,66,20,31,27,59,90,59,34,37,74,55,53,50,49,79,50,42,39,25,43,50,30,53,32,55,40,57,21,46,78,80,29,28,73,46,47,51,82,34,25,48,24,45,37,83,61,24,37,29,67,54,45,62,48,21,23,46,38,86,22,25,58,62,65,42,41,44,77,81,69,30,46,71,106,20,53,27,51,85,35,49,78,27,66,37,61,69,44,63,65,48,61,62,44,122,48,87,43,37,47,46,29,31,44,59,102,59,115,79,82,50,43,40,60,72,81,42,17,107,41,94,52,12,45,31,75,30,58,55,21,43,62,25,46,21,20,32,41,56,48,31,40,51,40,35,20,44,26,86,47,51,25,60,132,76,56,90,95,22,80,62,61,42,58,40,70,96,34,50,52,36,59,38,65,86,25,104,46,32,59,85,75,75,96,38,65,39,58,25,50,71,51,40,36,57,99,72,68,37,32,45,28,24,31,71,78,23,52,85,35,79,80,49,82,38,9,36,66,35,39,65,66,21,24,65,26,56,12,89,51,32,58,45,8,37,111,54,48,46,47,42,54,26,51,64,58,51,85,32,83,52,73,58,33,90,35,88,41,89,44,30,66,15,41,32,34,65,46,62,77,15,75,137,52,32,61,37,34,52,88,92,46,8,67,49,39,18,82,76,54,15,15,48,22,77,8,38,44,58,42,22,26,67,41,39,128,69,53,12,6,35,64,37,37,96,57,97,80,44,56,49,38,42,43,147,32,70,45,32,44,49,53,54,31,76,70,55,75,40,41,34,76,74,49,39,54,50,60,51,67,69,42,35,33,65,8,27,14,55,51,77,24,68,13,25,85,27,38,51,39,45,86,39,42,36,19,53,51,28,53,81,88,81,27,53,49,32,48,45,26,28,67,58,28,15,41,47,59,55,49,25,80,59,17,50,53,89,70,21,34,61,54,54,23,68,34,86,53,66,58,73,37,42,73,57,55,70,75,37,42,52,86,62,52,89,32,14,18,72,56,51,48,50,59,63,58,41,22,43,144,38,51,47,75,30,46,68,18,33,53,17,68,34,58,112,69,33,41,27,66,80,69,83,51,24,110,66,29,52,96,54,54,44,57,140,67,71,29,55,62,50,69,37,31,79,59,39,80,38,67,88,59,40,32,58,36,18,57,35,50,44,71,52,39,49,39,76,32,56,24,36,32,52,53,25,48,39,30,49,88,54,43,74,62,63,59,41,34,19,30,55,87,38,65,64,44,46,75,122,61,60,89,57,49,52,57,30,14,48,31,50,41,70,47,73,41,58,42,35,105,57,57,79,33,16,105,62,45,89,112,76,40,16,55,76,44,15,58,19,33,57,82,89,68,47,19,49,40,12,32,51,37,38,60,41,57,45,86,88,13,61,53,96,19,93,54,25,34,70,30,49,56,65,51,47,24,65,46,46,88,26,62,42,70,27,89,31,47,109,33,21,21,53,52,28,42,18,65,40,58,72,65,38,33,19,18,26,32,64,80,34,39,51,143,34,53,38,45,11,12,22,72,82,67,75,67,44,28,49,59,37,63,44,62,25,75,34,44,36,136,76,39,49,32,60,31,49,76,35,24,68,54,39,53,44,35,54,31,78,40,100,15,68,4,45,101,41,65,37,56,37,30,50,62,74,59,166,86,41,63,35,30,53,61,41,55,34,35,51,56,19,59,36,37,34,73,50,66,85,58,54,53,76,29,51,62,61,23,52,53,66,36,92,42,35,50,79,79,48,65,54,64,60,29,25,57,22,91,89,34,82,70,15,10,65,44,40,33,71,44,72,41,72,14,73,13,43,51,130,35,113,53,101,58,91,21,79,58,49,79,65,64,45,64,36,31,26,50,26,31,63,68,77,60,81,44,96,9,27,33,63,43,71,23,58,50,48,49,116,44,75,45,41,31,63,25,108,72,19,59,24,42,46,50,55,82,21,13,56,25,45,58,75,58,65,53,56,111,9,20,51,57,44,56,21,88,14,54,44,30,64,38,58,63,60,47,74,46,63,77,47,9,60,60,36,59,43,43,25,107,27,62,39,35,32,84,19,103,64,90,96,37,57,31,85,75,25,40,63,17,24,84,35,54,71,98,51,43,46,43,33,75,61,33,66,59,68,98,23,49,66,73,33,79,63,63,49,41,67,58,44,35,44,59,53,32,28,80,63,25,30,59,63,42,96,45,43,57,50,17,32,28,60,116,31,47,64,52,31,29,78,52,37,34,37,66,51,55,35,86,51,33,26,18,79,44,33,37,95,34,24,71,48,102,44,58,40,38,75,78,35,43,45,62,128,77,46,58,260,40,54,48,22,26,101,62,53,87,59,10,38,48,25,45,39,46,25,54,43,91,67,82,17,72,28,35,45,36,92,13,127,63,48,33,37,157,71,53,82,95,67,29,69,46,36,57,18,37,54,40,49,83,69,78,55,63,12,59,55,207,82,76,67,10,23,97,48,36,66,41,56,36,63,57,41,47,25,56,50,116,79,58,34,30,73,65,99,49,62,46,29,40,37,14,30,61,52,49,82,48,86,34,58,34,42,74,22,76,27,49,62,92,48,44,73,22,82,28,108,48,43,63,54,43,60,34,49,84,58,44,46,79,81,26,33,46,47,41,32,40,53,52,138,18,16,87,32,45,71,44,59,69,78,75,65,35,59,42,46,81,73,10,71,71,43,53,88,30,49,108,36,64,62,24,32,40,30,8,26,45,76,72,19,88,66,108,59,43,99,56,40,82,65,51,53,51,40,28,64,53,20,25,40,58,31,74,11,49,64,40,148,63,61,52,62,35,45,87,68,66,51,62,55,29,26,32,37,53,49,49,57,67,52,36,61,42,33,31,17,52,70,53,52,66,42,144,40,112,90,94,43,63,113,56,27,74,42,52,57,72,47,14,44,48,85,45,55,59,58,28,46,25,27,111,34,53,59,80,38,56,34,42,40,150,39,29,65,61,32,55,100,75,20,124,38,99,51,45,40,38,78,94,44,69,33,51,37,73,36,42,62,23,11,50,26,50,54,55,72,85,46,53,74,55,60,34,30,39,41,33,43,96,49,64,75,42,41,32,36,63,70,61,28,25,92,45,37,92,49,10,34,66,59,32,91,66,44,49,41,29,43,39,55,24,33,26,65,28,35,40,27,11,62,28,59,68,45,97,28,65,67,81,71,27,75,43,100,77,67,77,68,38,47,61,68,31,80,18,43,39,96,53,134,81,25,33,32,62,23,33,34,53,53,62,24,53,55,46,29,31,15,65,56,45,42,63,58,98,60,59,66,76,86,57,41,65,57,25,47,46,40,52,43,51,66,44,46,60,81,48,55,56,20,43,27,38,9,94,44,39,104,87,72,50,10,39,43,98,93,126,53,62,17,60,54,66,40,54,69,33,50,49,34,51,69,35,43,56,18,106,31,84,59,66,31,67,60,65,57,49,63,33,58,70,73,32,45,58,18,60,89,60,71,57,40,54,51,72,60,65,66,82,36,41,68,56,78,44,18,62,66,44,26,84,38,59,20,31,49,45,47,93,60,18,60,60,40,66,18,34,37,29,41,64,75,92,69,66,68,61,66,56,55,63,83,66,21,44,51,55,23,15,57,59,45,86,36,50,13,51,45,53,31,40,37,46,65,11,28,133,40,40,110,49,117,49,57,41,51,21,43,42,125,17,79,74,23,45,31,32,48,45,31,37,37,93,60,65,53,27,13,28,39,60,58,56,14,77,35,31,65,65,52,32,23,46,8,31,20,57,40,45,64,45,45,42,46,43,31,21,41,70,57,23,21,21,30,128,56,38,83,68,95,112,29,43,94,33,93,40,48,29,79,78,37,69,49,32,76,47,51,67,45,98,51,50,23,44,102,32,47,7,113,46,12,79,39,22,69,41,67,22,82,44,58,40,53,52,42,45,59,48,94,31,69,102,78,47,57,37,48,38,43,76,56,25,44,34,82,35,51,63,109,49,88,36,55,42,74,66,57,67,23,103,30,101,83,40,60,94,43,59,51,90,76,56,25,28,41,17,48,71,26,113,32,13,33,31,25,106,52,24,53,66,37,74,33,89,53,47,75,39,46,37,51,94,139,69,39,17,59,14,160,51,36,40,61,101,47,31,45,53,68,70,41,80,48,33,50,35,54,21,54,117,79,63,61,60,50,68,60,54,47,69,48,15,43,39,90,89,24,37,48,81,9,32,55,79,84,55,112,64,76,16,58,22,101,47,26,83,19,72,71,51,44,75,38,41,69,42,31,64,44,51,37,62,96,28,34,108,27,79,67,49,14,51,78,73,53,22,22,85,37,49,34,49,56,34,40,40,30,31,25,106,56,79,32,113,58,58,20,45,30,62,19,29,71,82,46,25,84,51,37,33,58,66,56,53,21,37,65,50,77,42,62,29,59,88,112,31,24,30,33,42,26,47,88,50,51,40,82,106,67,78,63,61,61,56,52,62,63,87,49,34,49,28,72,22,104,65,69,62,79,84,26,39,10,59,44,65,28,24,92,98,61,73,96,16,78,62,38,77,60,45,86,46,36,39,54,111,74,20,34,93,91,71,45,74,69,90,38,79,117,53,55,48,47,62,59,42,25,40,54,40,32,50,58,51,36,53,27,19,71,66,22,23,18,51,46,59,41,66,33,61,37,47,40,38,43,69,83,40,38,113,30,36,58,98,55,63,146,28,60,68,125,71,34,74,62,44,94,71,70,40,40,58,51,34,42,56,50,56,78,48,35,83,84,61,107,63,58,16,41,161,60,51,58,77,25,54,41,61,27,49,86,54,25,42,53,71,86,63,62,30,34,67,85,71,104,48,120,79,55,122,144,102,51,11,147,34,34,32,50,89,76,54,59,23,56,65,39,40,46,173,60,83,102,45,29,86,22,47,32,10,75,14,28,52,32,33,13,83,28,61,60,46,44,59,10,46,91,51,22,49,62,50,85,85,47,24,72,43,48,67,35,52,59,146,57,52,9,53,71,32,10,42,52,53,54,61,30,32,18,110,37,48,64,56,53,47,59,31,44,21,62,77,52,68,69,31,57,39,30,15,19,35,71,69,80,73,24,49,70,46,38,41,99,54,51,108,64,87,46,43,54,55,40,117,36,58,14,20,66,79,36,72,79,95,49,62,33,80,57,41,48,46,83,57,87,40,38,40,33,67,52,107,51,88,93,39,68,38,50,66,53,44,42,41,75,73,20,36,41,48,97,67,55,33,95,14,78,65,95,40,13,76,54,37,66,39,62,100,24,62,32,40,75,38,58,50,75,45,26,44,71,33,54,74,49,23,28,233,74,65,166,43,71,32,26,10,61,34,74,19,38,63,60,130,40,33,82,43,83,13,49,56,61,81,45,60,95,42,54,74,35,45,21,50,44,48,47,102,58,76,49,56,105,56,75,56,39,45,64,35,26,10,75,57,22,29,19,71,56,25,36,71,49,34,52,46,32,16,42,67,41,76,66,72,84,39,35,50,47,41,83,71,74,33,55,45,87,12,50,32,105,43,74,69,67,44,32,35,109,20,85,70,38,40,63,39,28,57,64,15,38,61,54,27,44,80,34,12,42,53,42,120,62,35,33,67,53,26,88,61,39,51,27,22,76,73,40,54,48,74,73,78,49,80,29,69,47,70,67,48,132,28,66,89,78,30,41,65,62,40,49,37,53,45,18,61,92,33,72,69,79,78,46,81,49,84,24,34,36,56,20,49,49,15,36,66,78,34,39,69,15,82,45,37,23,51,39,45,61,41,65,49,21,55,40,25,31,54,31,52,29,84,95,35,74,83,76,61,72,133,51,49,65,24,53,39,58,62,42,30,56,52,78,65,58,40,20,93,70,52,15,48,92,75,111,34,49,44,30,11,37,40,31,29,44,34,47,49,41,63,127,46,54,57,74,40,32,27,30,31,17,35,41,84,103,40,35,108,57,42,55,60,49,39,75,21,71,58,71,63,20,47,45,71,94,39,54,41,59,44,64,12,47,59,42,69,30,64,46,36,28,47,30,47,119,42,94,64,31,37,118,31,62,75,11,49,46,99,42,32,53,50,45,21,62,46,83,51,111,41,83,56,51,62,124,65,37,28,57,21,41,45,43,110,89,55,29,63,33,15,59,68,53,55,71,54,37,54,18,83,68,44,73,56,44,77,100,45,44,14,44,27,50,49,41,59,81,53,61,33,79,61,59,68,61,41,87,70,45,68,93,28,72,60,96,17,46,47,78,39,28,52,34,63,21,36,90,84,87,135,43,41,78,56,46,48,29,23,43,44,37,25,19,67,55,73,93,7,44,30,59,47,20,38,35,51,84,50,48,27,59,53,100,42,45,65,60,50,67,70,39,66,48,53,73,72,56,37,111,35,62,30,24,17,46,34,34,27,84,21,27,73,51,32,32,81,77,34,16,37,36,36,17,64,107,37,47,41,54,48,47,143,56,56,87,66,34,13,60,52,57,80,58,27,77,55,99,85,73,65,47,72,76,65,26,52,67,37,52,44,77,83,52,60,24,37,35,103,62,32,63,60,40,79,63,52,72,50,83,46,32,47,105,23,22,62,63,97,29,31,48,17,48,54,36,26,59,54,62,26,98,34,35,27,38,57,48,52,55,58,140,41,53,29,67,48,38,31,36,34,34,53,21,49,23,90,76,27,47,34,64,51,101,79,67,67,72,24,26,31,46,48,50,74,27,54,33,80,58,26,34,132,54,55,18,91,46,87,47,9,24,41,31,69,93,55,42,56,63,76,91,20,19,48,63,36,102,64,61,91,24,54,9,37,48,76,35,71,61,39,26,45,53,135,84,44,62,69,72,40,29,90,30,40,55,142,59,38,32,51,62,46,115,49,106,103,60,28,17,59,39,26,38,90,20,66,29,36,48,55,98,27,67,42,13,44,35,60,45,64,58,57,37,65,35,95,20,32,44,30,63,79,45,33,74,56,57,17,29,35,58,78,36,55,32,42,47,93,84,38,85,47,70,66,46,61,20,51,58,73,36,50,80,42,28,35,70,17,47,66,85,72,197,46,46,13,26,81,55,50,36,75,74,49,75,26,41,55,50,28,26,58,44,98,81,66,16,93,68,22,28,36,59,89,73,68,49,90,78,36,35,57,46,36,51,38,59,58,75,36,17,57,46,109,40,43,51,35,55,16,93,33,37,55,80,126,42,41,58,50,24,68,88,63,27,26,95,52,40,30,24,44,39,76,19,36,61,62,112,71,36,41,27,55,70,50,94,30,67,49,67,63,69,45,29,51,74,38,44,40,13,93,62,61,86,60,48,63,59,61,68,18,52,17,15,27,38,59,35,34,18,84,25,12,36,42,56,57,75,48,44,36,34,84,52,52,71,86,48,52,87,54,85,28,72,65,35,18,121,79,93,68,60,39,53,96,40,48,76,50,73,71,82,24,27,71,41,68,60,58,40,70,70,59,72,62,33,57,42,97,38,36,23,19,45,53,17,34,107,48,59,44,80,61,54,39,34,30,105,48,41,41,48,20,74,48,46,65,24,56,46,30,57,160,42,57,61,47,63,24,157,62,39,49,55,75,23,44,55,54,27,22,46,27,11,65,100,11,62,33,44,52,83,37,38,43,15,30,72,48,42,66,58,53,50,47,24,94,83,71,24,48,50,52,53,46,35,49,73,62,59,56,118,64,43,48,56,74,39,34,64,67,62,59,28,22,84,51,39,40,95,18,84,43,40,43,58,31,84,69,29,26,47,57,26,34,73,70,92,90,25,55,85,43,83,75,45,46,18,45,42,134,80,53,49,41,52,61,60,123,24,33,26,54,37,55,55,37,35,31,48,42,67,48,43,49,40,112,59,78,41,21,67,94,19,41,28,54,59,29,60,42,97,17,93,37,29,45,47,70,8,97,41,91,46,26,14,45,34,44,86,17,53,47,82,79,20,73,100,28,80,41,30,131,37,45,36,11,83,52,73,89,25,19,42,46,56,57,54,47,35,26,48,60,42,26,28,69,52,44,63,74,81,77,51,54,82,94,13,51,51,51,41,56,24,56,64,66,64,58,53,96,16,82,43,81,59,80,50,42,84,16,42,52,48,30,54,34,8,77,60,33,19,50,49,61,72,37,69,46,39,23,55,23,51,37,42,60,138,23,36,80,36,43,39,29,104,55,22,112,113,45,83,103,60,43,83,47,54,68,52,71,25,54,40,38,95,31,35,54,54,42,52,57,97,27,24,60,20,22,78,41,36,26,60,93,43,144,57,43,27,78,32,98,22,59,90,71,54,56,25,43,74,86,38,49,28,13,18,35,17,78,45,26,38,55,51,42,30,85,68,99,16,52,33,52,26,53,70,89,32,27,35,43,42,16,65,38,69,30,83,64,44,44,61,28,55,29,31,34,19,55,63,66,71,27,53,38,31,26,54,53,94,38,26,76,70,14,13,56,27,85,37,30,38,21,31,25,46,38,33,13,52,63,63,81,30,55,55,54,45,90,64,76,79,40,40,127,31,35,85,23,69,48,64,54,16,24,79,16,46,70,56,17,44,56,22,70,57,63,49,45,54,53,74,54,35,65,30,53,39,37,57,17,25,39,35,79,80,76,58,67,6,59,30,32,37,99,22,78,43,64,52,48,42,43,34,58,31,31,28,83,76,66,23,106,51,57,53,102,30,42,71,20,45,56,58,61,27,48,69,85,11,91,30,42,60,32,60,22,109,59,20,76,53,45,26,35,46,91,72,47,18,50,60,29,67,45,45,44,76,60,8,30,20,33,27,61,24,62,72,18,26,20,67,42,68,46,76,74,50,102,60,25,57,22,35,55,52,28,46,86,83,60,48,23,48,113,47,89,54,13,65,37,32,64,64,91,30,47,51,75,58,60,70,70,62,23,68,34,10,82,139,45,32,68,64,33,54,51,64,64,75,53,26,61,35,21,28,133,68,60,64,77,61,47,43,23,42,69,93,61,20,34,74,56,108,87,60,43,16,66,18,47,32,46,36,48,46,69,66,42,42,36,30,66,45,86,42,70,28,107,45,62,57,69,68,92,33,44,23,36,21,55,38,56,20,69,51,37,83,58,18,17,12,65,25,52,57,49,26,24,59,49,51,43,53,37,64,50,57,153,105,48,27,39,12,58,13,34,37,87,15,23,72,39,35,45,63,141,82,46,39,68,62,66,54,75,50,68,61,90,81,7,35,74,77,33,96,37,55,57,74,39,32,96,32,34,31,43,59,75,51,40,72,56,58,33,35,34,47,57,72,28,40,65,64,40,17,61,100,103,38,56,27,59,29,22,38,50,16,76,37,32,25,142,46,78,110,54,56,48,67,116,81,72,30,52,54,58,62,9,38,59,50,83,60,30,52,93,52,101,40,10,66,144,70,60,20,25,32,71,46,62,54,62,67,25,18,115,42,91,48,106,53,89,75,59,39,67,81,31,32,74,90,30,51,23,33,30,32,82,48,20,70,77,47,26,92,31,23,54,71,17,34,62,89,48,107,40,29,66,33,47,53,71,56,45,35,53,42,31,11,84,41,62,76,41,109,69,37,36,11,55,60,23,87,40,76,20,48,29,57,50,68,48,55,42,64,38,30,48,47,60,71,47,67,59,66,71,80,35,22,78,74,42,32,23,62,56,86,73,43,52,74,38,56,64,130,84,88,30,33,61,57,60,51,56,80,32,44,45],\"type\":\"histogram\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"title\":{\"text\":\"Text  distribution in the context of \\\"Hypothesis\\\"\"},\"xaxis\":{\"title\":{\"text\":\"Len of text in hypothesis\"}},\"yaxis\":{\"title\":{\"text\":\"Number of sentences\"}},\"bargap\":0.2,\"bargroupgap\":0.1},                        {\"responsive\": true}                    ).then(function(){\n                            \nvar gd = document.getElementById('0b94cf84-2188-4335-a62e-6183c46e621a');\nvar x = new MutationObserver(function (mutations, observer) {{\n        var display = window.getComputedStyle(gd).display;\n        if (!display || display === 'none') {{\n            console.log([gd, 'removed!']);\n            Plotly.purge(gd);\n            observer.disconnect();\n        }}\n}});\n\n// Listen for the removal of the full notebook cells\nvar notebookContainer = gd.closest('#notebook-container');\nif (notebookContainer) {{\n    x.observe(notebookContainer, {childList: true});\n}}\n\n// Listen for the clearing of the current output cell\nvar outputEl = gd.closest('.output');\nif (outputEl) {{\n    x.observe(outputEl, {childList: true});\n}}\n\n                        })                };                });            </script>        </div>"},"metadata":{}}]},{"cell_type":"markdown","source":"#  Setup","metadata":{}},{"cell_type":"code","source":"!pip install evaluate # library for metrics","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:14.857845Z","iopub.execute_input":"2023-08-09T20:20:14.858446Z","iopub.status.idle":"2023-08-09T20:20:29.349456Z","shell.execute_reply.started":"2023-08-09T20:20:14.858412Z","shell.execute_reply":"2023-08-09T20:20:29.348213Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Collecting evaluate\n  Downloading evaluate-0.4.0-py3-none-any.whl (81 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.4/81.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.1.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.23.5)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.6)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.5.3)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.31.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.65.0)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.2.0)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.14)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2023.6.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.16.4)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.18.0)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (9.0.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.8.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (3.12.2)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.6.3)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.0.9)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2023.5.7)\nRequirement already satisfied: python-dateutil>=2.8.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.1.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.3)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas->evaluate) (1.16.0)\nInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import evaluate\nimport torch\nfrom transformers import AdamW, AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, TrainingArguments, Trainer","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:29.351212Z","iopub.execute_input":"2023-08-09T20:20:29.351720Z","iopub.status.idle":"2023-08-09T20:20:38.361440Z","shell.execute_reply.started":"2023-08-09T20:20:29.351672Z","shell.execute_reply":"2023-08-09T20:20:38.360196Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"model_name = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli'\ntokenizer = AutoTokenizer.from_pretrained(model_name)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:38.363002Z","iopub.execute_input":"2023-08-09T20:20:38.363470Z","iopub.status.idle":"2023-08-09T20:20:44.340047Z","shell.execute_reply.started":"2023-08-09T20:20:38.363426Z","shell.execute_reply":"2023-08-09T20:20:44.337210Z"},"trusted":true},"execution_count":14,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)okenizer_config.json:   0%|          | 0.00/398 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"37469b4d0dd04e86a22c6a7c4f8a7528"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)tencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8075403a65d94e93a9ef7fa8d9a7364f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)/main/tokenizer.json:   0%|          | 0.00/9.08M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49f130f35a70418d8cba5ed1401bcaab"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (…)cial_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42f96573cdc1461f9e2484ccb98e6ca5"}},"metadata":{}}]},{"cell_type":"code","source":"train = train.drop(labels=['language', 'text_length', 'lang_abv'], axis=1)\ntest = test.drop(labels=['language','lang_abv'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:44.341459Z","iopub.execute_input":"2023-08-09T20:20:44.342240Z","iopub.status.idle":"2023-08-09T20:20:44.354519Z","shell.execute_reply.started":"2023-08-09T20:20:44.342201Z","shell.execute_reply":"2023-08-09T20:20:44.353494Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from datasets import Dataset, DatasetDict","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:44.356208Z","iopub.execute_input":"2023-08-09T20:20:44.356949Z","iopub.status.idle":"2023-08-09T20:20:44.402698Z","shell.execute_reply.started":"2023-08-09T20:20:44.356914Z","shell.execute_reply":"2023-08-09T20:20:44.401065Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ntrain_df, val_df = train_test_split(train, test_size=0.2, random_state=42)\n\ntrain_ds = Dataset.from_pandas(train_df)\nval_ds = Dataset.from_pandas(val_df)\ntest_ds = Dataset.from_pandas(test)\n\nds = DatasetDict()\nds['train'] = train_ds\nds['validation'] = val_ds\nds['test'] = test_ds\n\nds","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:44.404277Z","iopub.execute_input":"2023-08-09T20:20:44.404632Z","iopub.status.idle":"2023-08-09T20:20:44.517080Z","shell.execute_reply.started":"2023-08-09T20:20:44.404602Z","shell.execute_reply":"2023-08-09T20:20:44.516130Z"},"trusted":true},"execution_count":17,"outputs":[{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'label', '__index_level_0__'],\n        num_rows: 9696\n    })\n    validation: Dataset({\n        features: ['id', 'premise', 'hypothesis', 'label', '__index_level_0__'],\n        num_rows: 2424\n    })\n    test: Dataset({\n        features: ['id', 'premise', 'hypothesis'],\n        num_rows: 5195\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"def tokenizer_sentence(data):\n    return tokenizer(data['premise'], data['hypothesis'], truncation=True)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:44.519150Z","iopub.execute_input":"2023-08-09T20:20:44.520279Z","iopub.status.idle":"2023-08-09T20:20:44.529472Z","shell.execute_reply.started":"2023-08-09T20:20:44.520233Z","shell.execute_reply":"2023-08-09T20:20:44.527993Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"tokenized_ds = ds.map(tokenizer_sentence, batched=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:44.534743Z","iopub.execute_input":"2023-08-09T20:20:44.535314Z","iopub.status.idle":"2023-08-09T20:20:47.465058Z","shell.execute_reply.started":"2023-08-09T20:20:44.535280Z","shell.execute_reply":"2023-08-09T20:20:47.463616Z"},"trusted":true},"execution_count":19,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/10 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b968677e64494969a74a1d2aa64c350a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9ddf427568543a09a54d71e350f8c45"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/6 [00:00<?, ?ba/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85f693cdce094646aecb0c431be2ee21"}},"metadata":{}}]},{"cell_type":"code","source":"data_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:47.466951Z","iopub.execute_input":"2023-08-09T20:20:47.467453Z","iopub.status.idle":"2023-08-09T20:20:47.473691Z","shell.execute_reply.started":"2023-08-09T20:20:47.467409Z","shell.execute_reply":"2023-08-09T20:20:47.472418Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"markdown","source":"# Let's Build Model","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nfrom transformers import XLMRobertaModel","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:47.475558Z","iopub.execute_input":"2023-08-09T20:20:47.476043Z","iopub.status.idle":"2023-08-09T20:20:47.508140Z","shell.execute_reply.started":"2023-08-09T20:20:47.476004Z","shell.execute_reply":"2023-08-09T20:20:47.506891Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"class CustomXLMRobertaModel(nn.Module):\n    def __init__(self, num_labels):\n        super(CustomXLMRobertaModel, self).__init__()\n        model_name = 'symanto/xlm-roberta-base-snli-mnli-anli-xnli'\n        self.roberta = XLMRobertaModel.from_pretrained(model_name)\n        self.dropout = nn.Dropout(0.2)\n        self.classifier = nn.Sequential(\n            nn.Linear(768, 512),\n            nn.LayerNorm(512),\n            nn.ReLU(),\n            nn.Dropout(0.2),\n            nn.Linear(512, num_labels)\n        )\n        self.loss = nn.CrossEntropyLoss()\n        self.num_labels = num_labels\n\n    def forward(self, input_ids, attention_mask, labels=None):\n        output = self.roberta(input_ids=input_ids, attention_mask=attention_mask)\n        output = self.dropout(output.pooler_output)\n        logits = self.classifier(output)\n\n        if labels is not None:\n            loss = self.loss(logits.view(-1, self.num_labels), labels.view(-1))\n            return {\"loss\": loss, \"logits\": logits}\n        else:\n            return logits","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:47.509930Z","iopub.execute_input":"2023-08-09T20:20:47.510290Z","iopub.status.idle":"2023-08-09T20:20:47.521419Z","shell.execute_reply.started":"2023-08-09T20:20:47.510260Z","shell.execute_reply":"2023-08-09T20:20:47.519922Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"model = CustomXLMRobertaModel(num_labels=3) # we have 3 classes","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:47.523561Z","iopub.execute_input":"2023-08-09T20:20:47.524405Z","iopub.status.idle":"2023-08-09T20:20:58.424680Z","shell.execute_reply.started":"2023-08-09T20:20:47.524369Z","shell.execute_reply":"2023-08-09T20:20:58.423648Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading (…)lve/main/config.json:   0%|          | 0.00/921 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"947dad34878349d99f02649c73a84ce7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24faaea93e7246c2aa38b2186f25d3d3"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at symanto/xlm-roberta-base-snli-mnli-anli-xnli were not used when initializing XLMRobertaModel: ['classifier.out_proj.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.dense.weight']\n- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLMRobertaModel were not initialized from the model checkpoint at symanto/xlm-roberta-base-snli-mnli-anli-xnli and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Train Model","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score\nfrom datasets import load_metric\n\ntraining_args = TrainingArguments(\"/content\",\n                                  optim=\"adamw_torch\",\n                                  num_train_epochs=5,\n                                  evaluation_strategy=\"epoch\",\n                                  logging_dir='./logs',\n                                  logging_steps=10)\n\nf1_metric = load_metric(\"f1\")\n\ndef compute_metrics(eval_preds):\n    logits, labels = eval_preds\n    predictions = np.argmax(logits, axis=-1)\n    return {\n        'accuracy': accuracy_score(labels, predictions),\n        'f1': f1_metric.compute(predictions=predictions, references=labels, average=\"micro\")\n    }\n","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:58.426513Z","iopub.execute_input":"2023-08-09T20:20:58.427279Z","iopub.status.idle":"2023-08-09T20:20:59.139955Z","shell.execute_reply.started":"2023-08-09T20:20:58.427236Z","shell.execute_reply":"2023-08-09T20:20:59.138832Z"},"trusted":true},"execution_count":24,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/2.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf34b2a8b0b146eea6ac69b111c0a2c4"}},"metadata":{}}]},{"cell_type":"markdown","source":"Trainer provides a straightforward and fast way to train and evaluate your model.\n\ntrainer = Trainer(...: This creates an instance of the Trainer class, which will be used for training and evaluating the model. The arguments passed are:\n\nmodel: this is the model to be trained. In this context, it isn't defined yet, but it could be any model compatible with the transformers library.\n\nargs=training_args: these arguments control the training process. training_args should be an instance of TrainingArguments or compatible, which defines parameters like the learning rate, batch size, etc.\n\ntrain_dataset=tokenized_ds[\"train\"] and eval_dataset=tokenized_ds[\"validation\"]: these are the datasets for training and evaluating the model. In this case, they are taken from the dictionary tokenized_ds, presumably containing tokenized versions of the original data.\n\ndata_collator=data_collator: the data_collator is a function that takes a list of samples from the Dataset and collates them into mini-batches (batches) for training or evaluation. It isn't defined in this particular context.\n\ntokenizer=tokenizer: this is the tokenizer that was used to tokenize the input data.\n\ncompute_metrics=compute_metrics: this is a function that will be used to compute metrics during evaluation. In this context, it isn't defined yet. It should take the outputs from Trainer.evaluate() and return a dictionary where keys are the names of the metrics and values are the computed metrics.","metadata":{}},{"cell_type":"code","source":"from transformers import Trainer\n\ntrainer = Trainer(\n    model,\n    args=training_args,\n    train_dataset=tokenized_ds[\"train\"],\n    eval_dataset=tokenized_ds[\"validation\"],\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,  # передаем функцию compute_metrics\n)","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:59.141636Z","iopub.execute_input":"2023-08-09T20:20:59.142863Z","iopub.status.idle":"2023-08-09T20:20:59.194423Z","shell.execute_reply.started":"2023-08-09T20:20:59.142817Z","shell.execute_reply":"2023-08-09T20:20:59.193122Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Let's Setup env","metadata":{}},{"cell_type":"markdown","source":"Wandb, short for Weights & Biases, is a machine learning tool designed to monitor and visualize the progress of model training and facilitate the comparison of different experiments. It offers a user-friendly web interface that enables real-time observation of experiments, displays graphs depicting metrics like loss and accuracy, allows for the storage and retrieval of model weights, and permits easy sharing of results with collaborators.\n\nBy setting the environment variable `os.environ[\"WANDB_DISABLED\"] = \"true\"`, you can disable the integration with Weights & Biases. This can be beneficial if you prefer to avoid sending your experiments to Wandb or if you are working in an environment with restricted or no internet access. By doing so, you prevent any interactions or data exchange with Wandb, preserving the experiments within your local environment.","metadata":{}},{"cell_type":"code","source":"!pip install wandb","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:20:59.196234Z","iopub.execute_input":"2023-08-09T20:20:59.196595Z","iopub.status.idle":"2023-08-09T20:21:13.312391Z","shell.execute_reply.started":"2023-08-09T20:20:59.196566Z","shell.execute_reply":"2023-08-09T20:21:13.310981Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nRequirement already satisfied: wandb in /opt/conda/lib/python3.10/site-packages (0.15.5)\nRequirement already satisfied: Click!=8.0.0,>=7.1 in /opt/conda/lib/python3.10/site-packages (from wandb) (8.1.3)\nRequirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.1.31)\nRequirement already satisfied: requests<3,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (2.31.0)\nRequirement already satisfied: psutil>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (5.9.3)\nRequirement already satisfied: sentry-sdk>=1.0.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.27.1)\nRequirement already satisfied: docker-pycreds>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (0.4.0)\nRequirement already satisfied: PyYAML in /opt/conda/lib/python3.10/site-packages (from wandb) (6.0)\nRequirement already satisfied: pathtools in /opt/conda/lib/python3.10/site-packages (from wandb) (0.1.2)\nRequirement already satisfied: setproctitle in /opt/conda/lib/python3.10/site-packages (from wandb) (1.3.2)\nRequirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from wandb) (59.8.0)\nRequirement already satisfied: appdirs>=1.4.3 in /opt/conda/lib/python3.10/site-packages (from wandb) (1.4.4)\nRequirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /opt/conda/lib/python3.10/site-packages (from wandb) (3.20.3)\nRequirement already satisfied: six>=1.4.0 in /opt/conda/lib/python3.10/site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /opt/conda/lib/python3.10/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.0.0->wandb) (2023.5.7)\nRequirement already satisfied: smmap<6,>=3.0.1 in /opt/conda/lib/python3.10/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:21:13.314586Z","iopub.execute_input":"2023-08-09T20:21:13.315125Z","iopub.status.idle":"2023-08-09T20:21:13.321313Z","shell.execute_reply.started":"2023-08-09T20:21:13.315077Z","shell.execute_reply":"2023-08-09T20:21:13.320014Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2023-08-09T20:21:13.327867Z","iopub.execute_input":"2023-08-09T20:21:13.328493Z","iopub.status.idle":"2023-08-10T03:20:00.617547Z","shell.execute_reply.started":"2023-08-09T20:21:13.328459Z","shell.execute_reply":"2023-08-10T03:20:00.614879Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.15.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."},"metadata":{}},{"name":"stderr","text":"You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4510' max='6060' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4510/6060 6:57:53 < 2:23:40, 0.18 it/s, Epoch 3.72/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy</th>\n      <th>F1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.491800</td>\n      <td>0.574225</td>\n      <td>0.795380</td>\n      <td>{'f1': 0.7953795379537953}</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.503700</td>\n      <td>0.559343</td>\n      <td>0.804868</td>\n      <td>{'f1': 0.8048679867986799}</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.411400</td>\n      <td>0.877657</td>\n      <td>0.798680</td>\n      <td>{'f1': 0.7986798679867986}</td>\n    </tr>\n  </tbody>\n</table><p>\n    <div>\n      \n      <progress value='650' max='650' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [650/650 10:43]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Trainer is attempting to log a value of \"{'f1': 0.7953795379537953}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'f1': 0.8048679867986799}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\nTrainer is attempting to log a value of \"{'f1': 0.7986798679867986}\" of type <class 'dict'> for key \"eval/f1\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m1\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1 trainer.train()                                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2 \u001b[0m                                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1645\u001b[0m in \u001b[92mtrain\u001b[0m                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1642 \u001b[0m\u001b[2m│   │   \u001b[0minner_training_loop = find_executable_batch_size(                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1643 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m._inner_training_loop, \u001b[96mself\u001b[0m._train_batch_size, args.auto_find_batch_size  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1644 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1645 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m inner_training_loop(                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1646 \u001b[0m\u001b[2m│   │   │   \u001b[0margs=args,                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1647 \u001b[0m\u001b[2m│   │   │   \u001b[0mresume_from_checkpoint=resume_from_checkpoint,                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1648 \u001b[0m\u001b[2m│   │   │   \u001b[0mtrial=trial,                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m1938\u001b[0m in \u001b[92m_inner_training_loop\u001b[0m     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1935 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0m\u001b[96mself\u001b[0m.control = \u001b[96mself\u001b[0m.callback_handler.on_step_begin(args, \u001b[96mself\u001b[0m.state,  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1936 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1937 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.accelerator.accumulate(model):                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1938 \u001b[2m│   │   │   │   │   \u001b[0mtr_loss_step = \u001b[96mself\u001b[0m.training_step(model, inputs)                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1939 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1940 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[94mif\u001b[0m (                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1941 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0margs.logging_nan_inf_filter                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2759\u001b[0m in \u001b[92mtraining_step\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2756 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m loss_mb.reduce_mean().detach().to(\u001b[96mself\u001b[0m.args.device)                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2757 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2758 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mself\u001b[0m.compute_loss_context_manager():                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2759 \u001b[2m│   │   │   \u001b[0mloss = \u001b[96mself\u001b[0m.compute_loss(model, inputs)                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2760 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2761 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.n_gpu > \u001b[94m1\u001b[0m:                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2762 \u001b[0m\u001b[2m│   │   │   \u001b[0mloss = loss.mean()  \u001b[2m# mean() to average on multi-gpu parallel training\u001b[0m        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mtrainer.py\u001b[0m:\u001b[94m2784\u001b[0m in \u001b[92mcompute_loss\u001b[0m             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2781 \u001b[0m\u001b[2m│   │   │   \u001b[0mlabels = inputs.pop(\u001b[33m\"\u001b[0m\u001b[33mlabels\u001b[0m\u001b[33m\"\u001b[0m)                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2782 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2783 \u001b[0m\u001b[2m│   │   │   \u001b[0mlabels = \u001b[94mNone\u001b[0m                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m2784 \u001b[2m│   │   \u001b[0moutputs = model(**inputs)                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2785 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Save past state if it exists\u001b[0m                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2786 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# TODO: this needs to be fixed and made cleaner later.\u001b[0m                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m2787 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[96mself\u001b[0m.args.past_index >= \u001b[94m0\u001b[0m:                                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m in \u001b[92mforward\u001b[0m:\u001b[94m18\u001b[0m                                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mself\u001b[0m.num_labels = num_labels                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m16 \u001b[0m\u001b[2m│   \u001b[0m                                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, input_ids, attention_mask, labels=\u001b[94mNone\u001b[0m):                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m18 \u001b[2m│   │   \u001b[0moutput = \u001b[96mself\u001b[0m.roberta(input_ids=input_ids, attention_mask=attention_mask)           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   │   \u001b[0moutput = \u001b[96mself\u001b[0m.dropout(output.pooler_output)                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m20 \u001b[0m\u001b[2m│   │   \u001b[0mlogits = \u001b[96mself\u001b[0m.classifier(output)                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m21 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/\u001b[0m\u001b[1;33mmodeling_xlm_roberta.py\u001b[0m: \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m854\u001b[0m in \u001b[92mforward\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 851 \u001b[0m\u001b[2m│   │   │   \u001b[0minputs_embeds=inputs_embeds,                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 852 \u001b[0m\u001b[2m│   │   │   \u001b[0mpast_key_values_length=past_key_values_length,                                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 853 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 854 \u001b[2m│   │   \u001b[0mencoder_outputs = \u001b[96mself\u001b[0m.encoder(                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 855 \u001b[0m\u001b[2m│   │   │   \u001b[0membedding_output,                                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 856 \u001b[0m\u001b[2m│   │   │   \u001b[0mattention_mask=extended_attention_mask,                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 857 \u001b[0m\u001b[2m│   │   │   \u001b[0mhead_mask=head_mask,                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/\u001b[0m\u001b[1;33mmodeling_xlm_roberta.py\u001b[0m: \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m528\u001b[0m in \u001b[92mforward\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 525 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mencoder_attention_mask,                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 526 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m)                                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 527 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[94melse\u001b[0m:                                                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 528 \u001b[2m│   │   │   │   \u001b[0mlayer_outputs = layer_module(                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 529 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mhidden_states,                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 530 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mattention_mask,                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 531 \u001b[0m\u001b[2m│   │   │   │   │   \u001b[0mlayer_head_mask,                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/\u001b[0m\u001b[1;33mmodeling_xlm_roberta.py\u001b[0m: \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m454\u001b[0m in \u001b[92mforward\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 451 \u001b[0m\u001b[2m│   │   │   \u001b[0mcross_attn_present_key_value = cross_attention_outputs[-\u001b[94m1\u001b[0m]                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 452 \u001b[0m\u001b[2m│   │   │   \u001b[0mpresent_key_value = present_key_value + cross_attn_present_key_value          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 453 \u001b[0m\u001b[2m│   │   \u001b[0m                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 454 \u001b[2m│   │   \u001b[0mlayer_output = apply_chunking_to_forward(                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 455 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.feed_forward_chunk, \u001b[96mself\u001b[0m.chunk_size_feed_forward, \u001b[96mself\u001b[0m.seq_len_dim, att  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 456 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 457 \u001b[0m\u001b[2m│   │   \u001b[0moutputs = (layer_output,) + outputs                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/\u001b[0m\u001b[1;33mpytorch_utils.py\u001b[0m:\u001b[94m237\u001b[0m in                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[92mapply_chunking_to_forward\u001b[0m                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m234 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# concatenate output at same dimension\u001b[0m                                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m235 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m torch.cat(output_chunks, dim=chunk_dim)                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m236 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m237 \u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m forward_fn(*input_tensors)                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m238 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m239 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m240 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mfind_pruneable_heads_and_indices\u001b[0m(                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/\u001b[0m\u001b[1;33mmodeling_xlm_roberta.py\u001b[0m: \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m466\u001b[0m in \u001b[92mfeed_forward_chunk\u001b[0m                                                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 463 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m outputs                                                                    \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 464 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 465 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mfeed_forward_chunk\u001b[0m(\u001b[96mself\u001b[0m, attention_output):                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 466 \u001b[2m│   │   \u001b[0mintermediate_output = \u001b[96mself\u001b[0m.intermediate(attention_output)                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 467 \u001b[0m\u001b[2m│   │   \u001b[0mlayer_output = \u001b[96mself\u001b[0m.output(intermediate_output, attention_output)                 \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 468 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m layer_output                                                               \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 469 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/\u001b[0m\u001b[1;33mmodeling_xlm_roberta.py\u001b[0m: \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[94m364\u001b[0m in \u001b[92mforward\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 361 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[96mself\u001b[0m.intermediate_act_fn = config.hidden_act                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 362 \u001b[0m\u001b[2m│   \u001b[0m                                                                                      \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 363 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, hidden_states: torch.Tensor) -> torch.Tensor:                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 364 \u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.dense(hidden_states)                                         \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 365 \u001b[0m\u001b[2m│   │   \u001b[0mhidden_states = \u001b[96mself\u001b[0m.intermediate_act_fn(hidden_states)                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 366 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m hidden_states                                                              \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m 367 \u001b[0m                                                                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mmodule.py\u001b[0m:\u001b[94m1501\u001b[0m in \u001b[92m_call_impl\u001b[0m            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1498 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m (\u001b[96mself\u001b[0m._backward_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._backward_pre_hooks \u001b[95mor\u001b[0m \u001b[96mself\u001b[0m._forward_hooks   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1499 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_backward_pre_hooks \u001b[95mor\u001b[0m _global_backward_hooks                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1500 \u001b[0m\u001b[2m│   │   │   │   \u001b[0m\u001b[95mor\u001b[0m _global_forward_hooks \u001b[95mor\u001b[0m _global_forward_pre_hooks):                   \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m1501 \u001b[2m│   │   │   \u001b[0m\u001b[94mreturn\u001b[0m forward_call(*args, **kwargs)                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1502 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[2m# Do not call functions when jit is used\u001b[0m                                          \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1503 \u001b[0m\u001b[2m│   │   \u001b[0mfull_backward_hooks, non_full_backward_hooks = [], []                             \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m1504 \u001b[0m\u001b[2m│   │   \u001b[0mbackward_pre_hooks = []                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[2;33m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/\u001b[0m\u001b[1;33mlinear.py\u001b[0m:\u001b[94m114\u001b[0m in \u001b[92mforward\u001b[0m                \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m111 \u001b[0m\u001b[2m│   │   │   \u001b[0minit.uniform_(\u001b[96mself\u001b[0m.bias, -bound, bound)                                        \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m112 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m113 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mforward\u001b[0m(\u001b[96mself\u001b[0m, \u001b[96minput\u001b[0m: Tensor) -> Tensor:                                            \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m114 \u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m F.linear(\u001b[96minput\u001b[0m, \u001b[96mself\u001b[0m.weight, \u001b[96mself\u001b[0m.bias)                                     \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m115 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m116 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mextra_repr\u001b[0m(\u001b[96mself\u001b[0m) -> \u001b[96mstr\u001b[0m:                                                           \u001b[31m│\u001b[0m\n\u001b[31m│\u001b[0m   \u001b[2m117 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[94mreturn\u001b[0m \u001b[33m'\u001b[0m\u001b[33min_features=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m, out_features=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m, bias=\u001b[0m\u001b[33m{}\u001b[0m\u001b[33m'\u001b[0m.format(                          \u001b[31m│\u001b[0m\n\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n\u001b[1;91mKeyboardInterrupt\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1 trainer.train()                                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2 </span>                                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1645</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">train</span>                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1642 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>inner_training_loop = find_executable_batch_size(                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1643 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._inner_training_loop, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._train_batch_size, args.auto_find_batch_size  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1644 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1645 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> inner_training_loop(                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1646 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>args=args,                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1647 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>resume_from_checkpoint=resume_from_checkpoint,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1648 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>trial=trial,                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1938</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_inner_training_loop</span>     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1935 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.control = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.callback_handler.on_step_begin(args, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.state,  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1936 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1937 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.accelerator.accumulate(model):                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1938 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>tr_loss_step = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.training_step(model, inputs)                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1939 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1940 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> (                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1941 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>args.logging_nan_inf_filter                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2759</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">training_step</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2756 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> loss_mb.reduce_mean().detach().to(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.device)                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2757 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2758 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.compute_loss_context_manager():                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2759 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.compute_loss(model, inputs)                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2760 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2761 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.n_gpu &gt; <span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2762 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>loss = loss.mean()  <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># mean() to average on multi-gpu parallel training</span>        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">trainer.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">2784</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">compute_loss</span>             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2781 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>labels = inputs.pop(<span style=\"color: #808000; text-decoration-color: #808000\">\"labels\"</span>)                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2782 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2783 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>labels = <span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>2784 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = model(**inputs)                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2785 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Save past state if it exists</span>                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2786 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># TODO: this needs to be fixed and made cleaner later.</span>                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">2787 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.args.past_index &gt;= <span style=\"color: #0000ff; text-decoration-color: #0000ff\">0</span>:                                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">18</span>                                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.num_labels = num_labels                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">16 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, input_ids, attention_mask, labels=<span style=\"color: #0000ff; text-decoration-color: #0000ff\">None</span>):                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>18 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.roberta(input_ids=input_ids, attention_mask=attention_mask)           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dropout(output.pooler_output)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">20 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>logits = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.classifier(output)                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">21 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_xlm_roberta.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">854</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 851 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>inputs_embeds=inputs_embeds,                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 852 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>past_key_values_length=past_key_values_length,                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 853 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 854 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>encoder_outputs = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.encoder(                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 855 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>embedding_output,                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 856 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>attention_mask=extended_attention_mask,                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 857 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>head_mask=head_mask,                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_xlm_roberta.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">528</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 525 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>encoder_attention_mask,                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 526 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>)                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 527 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">else</span>:                                                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 528 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span>layer_outputs = layer_module(                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 529 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>hidden_states,                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 530 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>attention_mask,                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 531 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   │   </span>layer_head_mask,                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_xlm_roberta.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">454</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 451 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>cross_attn_present_key_value = cross_attention_outputs[-<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1</span>]                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 452 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>present_key_value = present_key_value + cross_attn_present_key_value          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 453 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 454 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>layer_output = apply_chunking_to_forward(                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 455 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.feed_forward_chunk, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.chunk_size_feed_forward, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.seq_len_dim, att  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 456 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>)                                                                                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 457 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>outputs = (layer_output,) + outputs                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">pytorch_utils.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">237</span> in                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">apply_chunking_to_forward</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">234 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># concatenate output at same dimension</span>                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">235 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> torch.cat(output_chunks, dim=chunk_dim)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">236 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>237 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_fn(*input_tensors)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">238 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">239 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">240 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">find_pruneable_heads_and_indices</span>(                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_xlm_roberta.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">466</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">feed_forward_chunk</span>                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 463 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> outputs                                                                    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 464 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 465 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">feed_forward_chunk</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, attention_output):                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 466 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>intermediate_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.intermediate(attention_output)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 467 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>layer_output = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.output(intermediate_output, attention_output)                 <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 468 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> layer_output                                                               <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 469 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/transformers/models/xlm_roberta/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">modeling_xlm_roberta.py</span>: <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #0000ff; text-decoration-color: #0000ff\">364</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 361 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.intermediate_act_fn = config.hidden_act                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 362 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 363 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, hidden_states: torch.Tensor) -&gt; torch.Tensor:                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 364 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.dense(hidden_states)                                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 365 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>hidden_states = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.intermediate_act_fn(hidden_states)                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 366 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> hidden_states                                                              <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 367 </span>                                                                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">module.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">1501</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">_call_impl</span>            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1498 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">if</span> <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">not</span> (<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>._forward_hooks   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1499 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_pre_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_backward_hooks                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1500 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   │   </span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_hooks <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">or</span> _global_forward_pre_hooks):                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>1501 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> forward_call(*args, **kwargs)                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1502 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"># Do not call functions when jit is used</span>                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1503 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>full_backward_hooks, non_full_backward_hooks = [], []                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">1504 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>backward_pre_hooks = []                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #bfbf7f; text-decoration-color: #bfbf7f\">/opt/conda/lib/python3.10/site-packages/torch/nn/modules/</span><span style=\"color: #808000; text-decoration-color: #808000; font-weight: bold\">linear.py</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">114</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">111 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   │   </span>init.uniform_(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias, -bound, bound)                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">112 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">113 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">forward</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>: Tensor) -&gt; Tensor:                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>114 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> F.linear(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">input</span>, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.weight, <span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>.bias)                                     <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">115 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">116 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">extra_repr</span>(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">self</span>) -&gt; <span style=\"color: #00ffff; text-decoration-color: #00ffff\">str</span>:                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">117 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> <span style=\"color: #808000; text-decoration-color: #808000\">'in_features={}, out_features={}, bias={}'</span>.format(                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">KeyboardInterrupt</span>\n</pre>\n"},"metadata":{}}]},{"cell_type":"markdown","source":"# Get Model predictions","metadata":{"execution":{"iopub.status.busy":"2023-08-10T03:20:00.619333Z","iopub.status.idle":"2023-08-10T03:20:00.621080Z","shell.execute_reply.started":"2023-08-10T03:20:00.620718Z","shell.execute_reply":"2023-08-10T03:20:00.620755Z"}}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"predictions = trainer.predict(tokenized_ds[\"test\"])\npredictions","metadata":{"execution":{"iopub.status.busy":"2023-08-10T03:20:52.831716Z","iopub.execute_input":"2023-08-10T03:20:52.832223Z","iopub.status.idle":"2023-08-10T03:31:37.159467Z","shell.execute_reply.started":"2023-08-10T03:20:52.832188Z","shell.execute_reply":"2023-08-10T03:31:37.158113Z"},"trusted":true},"execution_count":29,"outputs":[{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"PredictionOutput(predictions=array([[-2.7174554, -2.9699578,  4.2133455],\n       [-3.0694995,  4.623986 , -3.014065 ],\n       [ 4.928742 , -1.8966272, -2.1906483],\n       ...,\n       [ 4.900323 , -1.946845 , -1.9885854],\n       [ 4.8683796, -1.8874103, -1.9761918],\n       [-3.0400996, -2.7672114,  4.3198524]], dtype=float32), label_ids=None, metrics={'test_runtime': 644.3099, 'test_samples_per_second': 8.063, 'test_steps_per_second': 1.009})"},"metadata":{}}]},{"cell_type":"code","source":"logits = torch.from_numpy(predictions.predictions)\nprobs = torch.softmax(logits, -1).tolist() # convert to probability\nprobs[:5]","metadata":{"execution":{"iopub.status.busy":"2023-08-10T03:34:21.046663Z","iopub.execute_input":"2023-08-10T03:34:21.047274Z","iopub.status.idle":"2023-08-10T03:34:21.067286Z","shell.execute_reply.started":"2023-08-10T03:34:21.047228Z","shell.execute_reply":"2023-08-10T03:34:21.065487Z"},"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"[[0.0009755239589139819, 0.000757839996367693, 0.9982665777206421],\n [0.00045535998651757836, 0.9990634322166443, 0.00048131527728401124],\n [0.9981083869934082, 0.001083821291103959, 0.000807729025837034],\n [0.0005035892245359719, 0.9989799857139587, 0.000516395375598222],\n [0.003490425180643797, 0.995607316493988, 0.0009022981976158917]]"},"metadata":{}}]},{"cell_type":"code","source":"outputs = []\n\nfor index, prob in enumerate(probs):\n    # ind indx with max probability of class\n    predicted_label = prob.index(max(prob))\n    element_id = ds['test']['id'][index]\n    prediction = (element_id, predicted_label)\n    outputs.append(prediction)","metadata":{"execution":{"iopub.status.busy":"2023-08-10T03:34:24.659403Z","iopub.execute_input":"2023-08-10T03:34:24.659859Z","iopub.status.idle":"2023-08-10T03:35:07.568444Z","shell.execute_reply.started":"2023-08-10T03:34:24.659825Z","shell.execute_reply":"2023-08-10T03:35:07.566646Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"markdown","source":"# Save Submision¶\n","metadata":{}},{"cell_type":"code","source":"submission = pd.DataFrame(outputs, columns=['id', 'prediction'])\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2023-08-10T03:35:07.570969Z","iopub.execute_input":"2023-08-10T03:35:07.571364Z","iopub.status.idle":"2023-08-10T03:35:07.619147Z","shell.execute_reply.started":"2023-08-10T03:35:07.571334Z","shell.execute_reply":"2023-08-10T03:35:07.617877Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"           id  prediction\n0  c6d58c3f69           2\n1  cefcc82292           1\n2  e98005252c           0\n3  58518c10ba           1\n4  c32b0d16df           1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>c6d58c3f69</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>cefcc82292</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>e98005252c</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>58518c10ba</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>c32b0d16df</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Get Model predictions","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}